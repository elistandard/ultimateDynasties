{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup(url, delay=1, headers=None, proxy=None):\n",
    "    \"\"\"\n",
    "    Fetches and parses a webpage with BeautifulSoup, handling 403 errors\n",
    "    by setting headers, delays, and optional proxies.\n",
    "\n",
    "    Parameters:\n",
    "        url (str): The URL of the webpage to scrape.\n",
    "        delay (int): The delay (in seconds) between requests. Default is 1 second.\n",
    "        headers (dict): Optional headers to include in the request.\n",
    "        proxy (dict): Optional dictionary for proxies.\n",
    "\n",
    "    Returns:\n",
    "        BeautifulSoup object of the parsed page or None if request fails.\n",
    "    \"\"\"\n",
    "\n",
    "    # Default headers to mimic a browser request if none are provided\n",
    "    if headers is None:\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "        }\n",
    "\n",
    "    try:\n",
    "        # Send the request with headers and optional proxy\n",
    "        response = requests.get(url, headers=headers, proxies=proxy)\n",
    "        \n",
    "        # Check for 403 Forbidden error\n",
    "        if response.status_code == 403:\n",
    "            print(\"403 Forbidden: Access to the page is restricted.\")\n",
    "            return None\n",
    "        \n",
    "        # Add delay to avoid rapid requests\n",
    "        time.sleep(delay)\n",
    "\n",
    "        # Parse the content with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        return soup\n",
    "    \n",
    "    except requests.RequestException as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_list(urls, year):\n",
    "    try:\n",
    "        # Ensure the year exists in the provided URLs\n",
    "        if year not in urls:\n",
    "            print(f\"Year {year} not found in the URL dictionary.\")\n",
    "            return None\n",
    "        \n",
    "        # Get the URL for the given year\n",
    "        url = urls[year]\n",
    "\n",
    "        # Set headers to mimic a browser request\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "        }\n",
    "        \n",
    "        # Send a GET request to the URL\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Determine which instance of 'tablesorter' to use\n",
    "        tables = soup.find_all('table', class_='tablesorter')\n",
    "        if not tables:\n",
    "            print(f\"No tables with class 'tablesorter' found for {url}\")\n",
    "            return None\n",
    "        \n",
    "        # Choose the appropriate table based on the year\n",
    "        if (year <= 2000 and year not in [1998, 1999]) or (year >= 2013 and year <= 2019):\n",
    "            # Use the first instance of the table\n",
    "            data_table = tables[0] if len(tables) > 0 else None\n",
    "        else:\n",
    "            # Use the second instance of the table\n",
    "            data_table = tables[1] if len(tables) > 1 else None\n",
    "        \n",
    "        if not data_table:\n",
    "            print(f\"Appropriate data table not found for {url} and year {year}\")\n",
    "            return None\n",
    "\n",
    "        # Extract data from tbody\n",
    "        tbody = data_table.find('tbody')\n",
    "        if not tbody:\n",
    "            print(f\"No tbody found in the selected table for {url}\")\n",
    "            return None\n",
    "            \n",
    "        standings = []\n",
    "        for tr in tbody.find_all('tr'):\n",
    "            tds = tr.find_all('td')\n",
    "            if len(tds) >= 2:  # Assuming we need at least rank and team\n",
    "                rank = tds[0].text.strip()\n",
    "                team = tds[1].text.strip()\n",
    "                standings.append(f\"{rank} â€“ {team}\")\n",
    "\n",
    "        return standings\n",
    "\n",
    "    except requests.HTTPError as http_err:\n",
    "        print(f\"HTTP error occurred while scraping {year}: {http_err}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while scraping {year}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_multiple_urls_dict(urls):\n",
    "    # Dictionary to store the results for each year\n",
    "    results = {}\n",
    "    for year, url in urls.items():\n",
    "        print(f\"Scraping data for the year {year} from {url}...\")\n",
    "        # Pass the urls dictionary and the year to scrape_list\n",
    "        data = scrape_list(urls, year)\n",
    "        results[year] = data if data else \"No data found\"\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = {\n",
    "    1979: 'https://archive.usaultimate.org/archives/1979_club.aspx',\n",
    "    1980: 'https://archive.usaultimate.org/archives/1980_club.aspx',\n",
    "    1981: 'https://archive.usaultimate.org/archives/1981_club.aspx',\n",
    "    1982: 'https://archive.usaultimate.org/archives/1982_club.aspx',\n",
    "    1983: 'https://archive.usaultimate.org/archives/1983_club.aspx',\n",
    "    1984: 'https://archive.usaultimate.org/archives/1984_club.aspx',\n",
    "    1985: 'https://archive.usaultimate.org/archives/1985_club.aspx',\n",
    "    1986: 'https://archive.usaultimate.org/archives/1986_club.aspx',\n",
    "    1987: 'https://archive.usaultimate.org/archives/1987_club.aspx',\n",
    "    1988: 'https://archive.usaultimate.org/archives/1988_club.aspx',\n",
    "    1989: 'https://archive.usaultimate.org/archives/1989_club.aspx',\n",
    "    1990: 'https://archive.usaultimate.org/archives/1990_club.aspx',\n",
    "    1991: 'https://archive.usaultimate.org/archives/1991_club.aspx',\n",
    "    1992: 'https://archive.usaultimate.org/archives/1992_club.aspx',\n",
    "    1993: 'https://archive.usaultimate.org/archives/1993_club.aspx',\n",
    "    1994: 'https://archive.usaultimate.org/archives/1994_club.aspx',\n",
    "    1995: 'https://archive.usaultimate.org/archives/1995_club.aspx',\n",
    "    1996: 'https://archive.usaultimate.org/archives/1996_club.aspx',\n",
    "    1997: 'https://archive.usaultimate.org/archives/1997_club.aspx',\n",
    "    1998: 'https://archive.usaultimate.org/archives/1998_club.aspx',\n",
    "    1999: 'https://archive.usaultimate.org/archives/1999_club.aspx',\n",
    "    2000: 'https://archive.usaultimate.org/archives/2000_club.aspx',\n",
    "    2001: 'https://archive.usaultimate.org/archives/2001_club.aspx',\n",
    "    2002: 'https://archive.usaultimate.org/archives/2002_club.aspx',\n",
    "    2003: 'https://archive.usaultimate.org/archives/2003_club.aspx',\n",
    "    2004: 'https://archive.usaultimate.org/archives/2004_club.aspx',\n",
    "    2005: 'https://archive.usaultimate.org/archives/2005_club.aspx',\n",
    "    2006: 'https://archive.usaultimate.org/archives/2006_club.aspx',\n",
    "    2007: 'https://archive.usaultimate.org/archives/2007_club.aspx',\n",
    "    2008: 'https://archive.usaultimate.org/archives/2008_club.aspx',\n",
    "    2009: 'https://archive.usaultimate.org/archives/2009_club.aspx',\n",
    "    2010: 'https://archive.usaultimate.org/archives/2010_club.aspx',\n",
    "    2011: 'https://archive.usaultimate.org/archives/2011_club.aspx',\n",
    "    2012: 'https://archive.usaultimate.org/archives/2012_club.aspx',\n",
    "    2013: 'https://archive.usaultimate.org/archives/2013_club.aspx',\n",
    "    2014: 'https://archive.usaultimate.org/archives/2014_club.aspx',\n",
    "    2015: 'https://archive.usaultimate.org/archives/2015_club.aspx',\n",
    "    2016: 'https://archive.usaultimate.org/archives/2016_club.aspx',\n",
    "    2017: 'https://archive.usaultimate.org/archives/2017_club.aspx',\n",
    "    2018: 'https://archive.usaultimate.org/archives/2018_club.aspx',\n",
    "    2019: 'https://archive.usaultimate.org/archives/2019_club.aspx'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for the year 1979 from https://archive.usaultimate.org/archives/1979_club.aspx...\n",
      "Scraping data for the year 1980 from https://archive.usaultimate.org/archives/1980_club.aspx...\n",
      "Scraping data for the year 1981 from https://archive.usaultimate.org/archives/1981_club.aspx...\n",
      "Scraping data for the year 1982 from https://archive.usaultimate.org/archives/1982_club.aspx...\n",
      "Scraping data for the year 1983 from https://archive.usaultimate.org/archives/1983_club.aspx...\n",
      "Scraping data for the year 1984 from https://archive.usaultimate.org/archives/1984_club.aspx...\n",
      "Scraping data for the year 1985 from https://archive.usaultimate.org/archives/1985_club.aspx...\n",
      "Scraping data for the year 1986 from https://archive.usaultimate.org/archives/1986_club.aspx...\n",
      "Scraping data for the year 1987 from https://archive.usaultimate.org/archives/1987_club.aspx...\n",
      "Scraping data for the year 1988 from https://archive.usaultimate.org/archives/1988_club.aspx...\n",
      "Scraping data for the year 1989 from https://archive.usaultimate.org/archives/1989_club.aspx...\n",
      "Scraping data for the year 1990 from https://archive.usaultimate.org/archives/1990_club.aspx...\n",
      "Scraping data for the year 1991 from https://archive.usaultimate.org/archives/1991_club.aspx...\n",
      "Scraping data for the year 1992 from https://archive.usaultimate.org/archives/1992_club.aspx...\n",
      "Scraping data for the year 1993 from https://archive.usaultimate.org/archives/1993_club.aspx...\n",
      "Scraping data for the year 1994 from https://archive.usaultimate.org/archives/1994_club.aspx...\n",
      "Scraping data for the year 1995 from https://archive.usaultimate.org/archives/1995_club.aspx...\n",
      "Scraping data for the year 1996 from https://archive.usaultimate.org/archives/1996_club.aspx...\n",
      "Scraping data for the year 1997 from https://archive.usaultimate.org/archives/1997_club.aspx...\n",
      "Scraping data for the year 1998 from https://archive.usaultimate.org/archives/1998_club.aspx...\n",
      "Scraping data for the year 1999 from https://archive.usaultimate.org/archives/1999_club.aspx...\n",
      "Scraping data for the year 2000 from https://archive.usaultimate.org/archives/2000_club.aspx...\n",
      "Scraping data for the year 2001 from https://archive.usaultimate.org/archives/2001_club.aspx...\n",
      "Scraping data for the year 2002 from https://archive.usaultimate.org/archives/2002_club.aspx...\n",
      "Scraping data for the year 2003 from https://archive.usaultimate.org/archives/2003_club.aspx...\n",
      "Scraping data for the year 2004 from https://archive.usaultimate.org/archives/2004_club.aspx...\n",
      "Scraping data for the year 2005 from https://archive.usaultimate.org/archives/2005_club.aspx...\n",
      "Scraping data for the year 2006 from https://archive.usaultimate.org/archives/2006_club.aspx...\n",
      "Scraping data for the year 2007 from https://archive.usaultimate.org/archives/2007_club.aspx...\n",
      "Scraping data for the year 2008 from https://archive.usaultimate.org/archives/2008_club.aspx...\n",
      "Scraping data for the year 2009 from https://archive.usaultimate.org/archives/2009_club.aspx...\n",
      "Scraping data for the year 2010 from https://archive.usaultimate.org/archives/2010_club.aspx...\n",
      "Scraping data for the year 2011 from https://archive.usaultimate.org/archives/2011_club.aspx...\n",
      "Scraping data for the year 2012 from https://archive.usaultimate.org/archives/2012_club.aspx...\n",
      "Scraping data for the year 2013 from https://archive.usaultimate.org/archives/2013_club.aspx...\n",
      "Scraping data for the year 2014 from https://archive.usaultimate.org/archives/2014_club.aspx...\n",
      "Scraping data for the year 2015 from https://archive.usaultimate.org/archives/2015_club.aspx...\n",
      "Scraping data for the year 2016 from https://archive.usaultimate.org/archives/2016_club.aspx...\n",
      "Scraping data for the year 2017 from https://archive.usaultimate.org/archives/2017_club.aspx...\n",
      "Scraping data for the year 2018 from https://archive.usaultimate.org/archives/2018_club.aspx...\n",
      "Scraping data for the year 2019 from https://archive.usaultimate.org/archives/2019_club.aspx...\n"
     ]
    }
   ],
   "source": [
    "all_standings = scrape_multiple_urls_dict(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Standings for 1979:\n",
      "['1 â€“ Glassboro', '2 â€“ Condors', '3 â€“ Michigan State', '4 â€“ Orlando', '5 â€“ Cornell']\n",
      "\n",
      "Standings for 1980:\n",
      "['1 â€“ Glassboro', '2 â€“ Aerodisc', '3 â€“ Michigan State', '4 â€“ Condors', '5 â€“ Sky Pilots']\n",
      "\n",
      "Standings for 1981:\n",
      "['1 â€“ Condors', '2 â€“ Knights of Nee', '3 â€“ Sky Pilots', '4 â€“ Hostages', '5 â€“ Michigan State']\n",
      "\n",
      "Standings for 1982:\n",
      "['1 â€“ Rude Boys', '2 â€“ Tunas', '3 â€“ Gang', '4 â€“ Hostages', '5 â€“ Windy City', '6 â€“ Sky Pilots', '7 â€“ Chain Lightning', '8 â€“ Glassboro', '9 â€“ Fun Hogs', '10 â€“ Flying Circus']\n",
      "\n",
      "Standings for 1983:\n",
      "['1 â€“ Windy City', '2 â€“ Spinoffs', '3T â€“ Sky Pilots', '3T â€“ Condors', '5T â€“ Tunas', '5T â€“ Rude Boys', '7T â€“ The Gang', '7T â€“ Static Disc', '9T â€“ Kaboom', '9T â€“ Fat Women']\n",
      "\n",
      "Standings for 1984:\n",
      "['1 â€“ Tunas', '2 â€“ Flying Circus', '3T â€“ Windy City', '3T â€“ Condors', '5T â€“ Kaboom', '5T â€“ Rude Boys', '7T â€“ Miami Refugees', '7T â€“ Fat Women', '9T â€“ Static Disc', '9T â€“ The Gang']\n",
      "\n",
      "Standings for 1985:\n",
      "['1 â€“ Flying Circus', '2 â€“ Kaboom', '3T â€“ Windy City', '3T â€“ Tunas', '5T â€“ Titanic', '5T â€“ Rivals', '7T â€“ Mr. Pouce', '7T â€“ Condors', '9T â€“ COGZ', '9T â€“ Chain Lightning']\n",
      "\n",
      "Standings for 1986:\n",
      "['1 â€“ Windy City', '2 â€“ Flying Circus', '3T â€“ Titanic', '3T â€“ Kaboom', '5T â€“ Tunas', '5T â€“ Condors', '7T â€“ Texas Heat', '7T â€“ Chain Lightning', '9T â€“ Mr. Pouce', '9T â€“ R & B']\n",
      "\n",
      "Standings for 1987:\n",
      "['1 â€“ New York', '2 â€“ Windy City', '3T â€“ Tunas', '3T â€“ Titanic', '5T â€“ Tsunami', '5T â€“ Condors', '7T â€“ Looney Tunes', '7T â€“ Chain Lightning', '9T â€“ Yo Mama', '9T â€“ Drivers', '11T â€“ Mr. Bubble', '11T â€“ Texas Heat']\n",
      "\n",
      "Standings for 1988:\n",
      "['1 â€“ Tsunami', '2 â€“ Titanic', '3T â€“ Windy City', '3T â€“ New York', '5T â€“ Looney Tunes', '5T â€“ Condors', '7T â€“ Grafitti', '7T â€“ Polo Club', '9T â€“ Philmore', '9T â€“ Miami', '11T â€“ Tunas', '11T â€“ Splat']\n",
      "\n",
      "Standings for 1989:\n",
      "['1 â€“ New York', '2 â€“ Tsunami', '3T â€“ Titanic', '3T â€“ Iguanas', '5T â€“ Windy City', '5T â€“ Oregon Donors', '7T â€“ Amanata', '7T â€“ Philmore', '9T â€“ Looney Tunes', '9T â€“ Chain Lightning', '11T â€“ Earth Atomizer', '11T â€“ Jaga']\n",
      "\n",
      "Standings for 1990:\n",
      "['1 â€“ New York', '2 â€“ Iguanas', '3T â€“ Windy City', '3T â€“ First Time Gary', '5T â€“ Earth Atomizer', '5T â€“ Tsunami', '7T â€“ Amanata', '7T â€“ Arm & Hammer', '9T â€“ Chain Lightning', '9T â€“ Condors', '11T â€“ Electric Pig', '11T â€“ Vicious Cycle']\n",
      "\n",
      "Standings for 1991:\n",
      "['1 â€“ New York', '2 â€“ Big Brother', '3T â€“ Iguanas', '3T â€“ South Bay', '5T â€“ Graffiti', '5T â€“ East Bay', '7T â€“ Windy City', '7T â€“ Refugees', '9T â€“ Electric Pig', '9T â€“ Philmore', '11T â€“ Team \"O\"', '11T â€“ Chain Lightning']\n",
      "\n",
      "Standings for 1992:\n",
      "['1 â€“ New York', '2 â€“ Commonwealth', '3T â€“ Windy City', '3T â€“ Rhino Slam!', '5T â€“ Ring of Fire', '5T â€“ East Bay', '7T â€“ Graffiti', '7T â€“ Vicious Cycle', '9T â€“ Refugees', '9T â€“ Iguanas', '11T â€“ Night Train', '11T â€“ Port City Slickers']\n",
      "\n",
      "Standings for 1993:\n",
      "['1 â€“ New York', '2 â€“ Double Happiness', '3T â€“ Big Brother', '3T â€“ Rhino Slam!', '5T â€“ Night Train', '5T â€“ Ring of Fire', '7T â€“ Refugees', '7T â€“ South Coast', '9T â€“ Windy City', '9T â€“ Trouble Club', '11T â€“ Graffiti', '11T â€“ Chain Lightning']\n",
      "\n",
      "Standings for 1994:\n",
      "['1 â€“ Death or Glory', '2 â€“ Double Happiness', '3T â€“ Chesapeake', '3T â€“ Cojones', '5T â€“ Z', '5T â€“ Ring of Fire', '7T â€“ Big River', '7T â€“ Boulder Stains', '9T â€“ Refugees', '9T â€“ Chain Lightning', '11T â€“ Cornell', '11T â€“ Rhino Slam!']\n",
      "\n",
      "Standings for 1995:\n",
      "['1 â€“ Death or Glory', '2 â€“ Sockeye', '3T â€“ Cojones', '3T â€“ Double Happiness', '5T â€“ Port City Slickers', '5T â€“ Chain Lightning', '7T â€“ Refugees', '7T â€“ San Diego', '9T â€“ Z', '9T â€“ Ring of Fire', '11T â€“ Lemon', '11T â€“ Cornell']\n",
      "\n",
      "Standings for 1996:\n",
      "['1 â€“ Death or Glory', '2 â€“ Sockeye', '3T â€“ Z', '3T â€“ Nice Guys', '5T â€“ Refugees', '5T â€“ Chain Lightning', '7T â€“ Rage', '7T â€“ Snapple', '9T â€“ Boulder Beer', '9T â€“ Saucy Jack', \"11T â€“ Randall's Island\", \"11T â€“ Huckin' Foosiers\", '13T â€“ Port City Slickers', '13T â€“ Big Ass Truck']\n",
      "\n",
      "Standings for 1997:\n",
      "['1 â€“ Death or Glory', '2 â€“ Sockeye', '3T â€“ Z', '3T â€“ Ring of Fire', '5T â€“ WUDI', '5T â€“ Double Happiness', '7T â€“ Anodyne', '7T â€“ Condors', '9T â€“ Pump House 5', '9T â€“ RoQ', '11T â€“ Chain Lightning', '11T â€“ Johnny Bravo', '13T â€“ Big Ass Truck', '13T â€“ Houndz']\n",
      "\n",
      "Standings for 1998:\n",
      "['1 â€“ Death or Glory', '2 â€“ Condors', '3T â€“ Ring of Fire', '3T â€“ WSL', '5T â€“ Furious George', '5T â€“ Houndz', '7T â€“ Sub Zero', '7T â€“ Parking Lot JAM', '9T â€“ Z', '9T â€“ Miami Refugees', '11T â€“ Rage', '11T â€“ Los Guapos', '13T â€“ Pump House 5', '13T â€“ Red Tide']\n",
      "\n",
      "Standings for 1999:\n",
      "['1 â€“ Death or Glory', '2 â€“ Condors', '3T â€“ Sub Zero', '3T â€“ Furious George', '5T â€“ Ring of Fire', '5T â€“ JAM', '5T â€“ Blaze of Glory', '5T â€“ Houndz', '9 â€“ Madison', '10 â€“ Anodyne', '11 â€“ WSL', '12 â€“ Florida', '13 â€“ Secret Squirrel', '14 â€“ Booty Quake', '15 â€“ Second Wind', '16 â€“ Red Tide']\n",
      "\n",
      "Standings for 2000:\n",
      "['1 â€“ Condors', '2 â€“ Furious George', '3T â€“ Death or Glory', '3T â€“ JAM', '5 â€“ Ring of Fire', '6 â€“ Florida', '7 â€“ Sockeye', '8 â€“ Sub Zero', '9 â€“ FBI', '10 â€“ Bomb Squad', '11 â€“ Cbass', '12 â€“ Madison', '13 â€“ Second Wind', '14 â€“ Johnny Bravo', '15 â€“ Houndz', '16 â€“ Blackjack']\n",
      "\n",
      "Standings for 2001:\n",
      "['1 â€“ Condors', '2 â€“ Jam', '3T â€“ Death or Glory', '3T â€“ Furious George', '5 â€“ Sub Zero', '6 â€“ Johnny Bravo', '7 â€“ Electric Pig', '8 â€“ New York', '9 â€“ Sockeye', '10 â€“ Bonzi', '11 â€“ Madison', '12 â€“ Warriors', '13 â€“ Ring of Fire', '14 â€“ Florida', '15 â€“ Big Ass Truck', '16 â€“ Doublewide']\n",
      "\n",
      "Standings for 2002:\n",
      "['1 â€“ Furious George', '2 â€“ Ring of Fire', '3T â€“ Death or Glory', '3T â€“ Sockeye', '5T â€“ Condors', '5T â€“ Johnny Bravo', '7T â€“ Chain Lightning', '7T â€“ Sub Zero', '9 â€“ Boss Hogg', '10 â€“ Pike', '11 â€“ Vicious Cycle', '12 â€“ Machine', '13 â€“ New York', '14 â€“ PBR Streetgang', '15 â€“ Electric Pig', '16 â€“ Madison']\n",
      "\n",
      "Standings for 2003:\n",
      "['1 â€“ Furious George', '2 â€“ Condors', '3T â€“ Jam', '3T â€“ Ring of Fire', '5T â€“ Death or Glory', '5T â€“ Sockeye', '7T â€“ Doublewide', '7T â€“ Johnny Bravo', '9 â€“ G-Unit', '10 â€“ Chain Lightning', '11 â€“ Boss Hogg', '12 â€“ Vicious Cycle', '13 â€“ Big Ass Truck', '14 â€“ Pike', '15 â€“ Electric Pig', '16 â€“ Sub Zero']\n",
      "\n",
      "Standings for 2004:\n",
      "['1 â€“ Sockeye', '2 â€“ Jam', '3T â€“ Furious George', '3T â€“ Pike', '5 â€“ Death or Glory', '6 â€“ Johnny Bravo', '7T â€“ Condors', '7T â€“ Ring of Fire', '9 â€“ Big Ass Truck', '10 â€“ Doublewide', '11 â€“ Kaos', '12 â€“ Chain Lightning', '13 â€“ Sub Zero', '14 â€“ Electric Pig', '15 â€“ GOAT', '16 â€“ Machine']\n",
      "\n",
      "Standings for 2005:\n",
      "['1 â€“ Furious George', '2 â€“ Sockeye', '3T â€“ Jam', '3T â€“ Death or Glory', '5 â€“ Johnny Bravo', '6 â€“ Sub Zero', '7 â€“ Pike', '8 â€“ Doublewide', '9 â€“ Condors', '10 â€“ Potomac', '11 â€“ Metal', '12 â€“ Big Ass Truck', '13 â€“ Ring of Fire', '14 â€“ Chain Lightning', '15 â€“ Vicious Cycle', '16 â€“ PBR Streetgang']\n",
      "\n",
      "Standings for 2006:\n",
      "['1 â€“ Sockeye', '2 â€“ Furious George', '3T â€“ Chain Lightning', '3T â€“ Johnny Bravo', '5 â€“ Revolver', '6 â€“ Rhino', '7T â€“ Death or Glory', '7T â€“ Ring of Fire', '9 â€“ Metal', '10 â€“ Sub Zero', '11 â€“ Condors', '12 â€“ Machine', '13 â€“ Vicious Cycle', '14 â€“ Truck Stop', '15 â€“ BAT', '16 â€“ Monster']\n",
      "\n",
      "Standings for 2007:\n",
      "['1 â€“ Sockeye', '2 â€“ Johnny Bravo', '3T â€“ GOAT', '3T â€“ Jam', '5T â€“ Boston Ultimate', '5T â€“ Truck Stop', '7T â€“ Chain Lightning', '7T â€“ Sub Zero', '9 â€“ Furious George', '10 â€“ Condors', '11 â€“ Ring of Fire', '12 â€“ Machine', '13 â€“ Rhino', '14 â€“ Doublewide', '15 â€“ Pike', '16 â€“ The Van Buren Boys']\n",
      "\n",
      "Standings for 2008:\n",
      "['1 â€“ Jam', '2 â€“ Ironside', '3T â€“ Chain Lightning', '3T â€“ Johnny Bravo', '5 â€“ Revolver', '6 â€“ GOAT', '7T â€“ Ring of Fire', '7T â€“ Sockeye', '9 â€“ Sub Zero', '10 â€“ Machine', '11 â€“ Doublewide', '12 â€“ Bodhi', '13 â€“ Condors', '14 â€“ PoNY', '15 â€“ Truck Stop', '16 â€“ El Diablo']\n",
      "\n",
      "Standings for 2009:\n",
      "['1 â€“ Chain Lightning', '2 â€“ Revolver', '3T â€“ Ironside', '3T â€“ Sockeye', '5 â€“ Doublewide', '6 â€“ Ring of Fire', '7 â€“ Jam', '8 â€“ Johnny Bravo', '9 â€“ Truck Stop', '10 â€“ Madison Club', '11 â€“ Bohdi', '12 â€“ GOAT', '13 â€“ Madcow', '14 â€“ Machine', '15 â€“ Streetgang', '16 â€“ Pike']\n",
      "\n",
      "Standings for 2010:\n",
      "['1 â€“ Revolver', '2 â€“ Ironside', '3T â€“ Doublewide', '3T â€“ Sockeye', '5T â€“ Chain Lightning', '5T â€“ Truck Stop', '7T â€“ Ring of Fire', '7T â€“ Southpaw', '9 â€“ Johnny Bravo', '10 â€“ PoNY', '11 â€“ Madison Club', '12 â€“ Furious George', '13 â€“ Machine', '14 â€“ Tanasi', '15 â€“ Streetgang', '16 â€“ Madcow']\n",
      "\n",
      "Standings for 2011:\n",
      "['1 â€“ Revolver', '2 â€“ Ironside', '3T â€“ Chain Lightning', '3T â€“ Doublewide', '5 â€“ Ring of Fire', '6 â€“ Southpaw', '7 â€“ Madison Club', '8 â€“ GOAT', '9 â€“ Machine', '10 â€“ Truck Stop', '11 â€“ Johnny Bravo', '12 â€“ Oakland', '13 â€“ Furious George', '14 â€“ Condors', '15 â€“ Sub Zero', '16 â€“ Tanasi']\n",
      "\n",
      "Standings for 2012:\n",
      "['1 â€“ Doublewide', '2 â€“ Revolver', '3T â€“ Ironside', '3T â€“ Ring of Fire', '5 â€“ Machine', '6 â€“ GOAT', '7T â€“ Chain Lightning', '7T â€“ Sockeye', '9 â€“ Madison Club', '10 â€“ Rhino', '11 â€“ Furious George', '12 â€“ Johnny Bravo', '13T â€“ Sub Zero', '13T â€“ Truck Stop', '15 â€“ Boost Mobile', '16 â€“ PoNY']\n",
      "\n",
      "Standings for 2013:\n",
      "['1 â€“ Revolver', '2 â€“ Sockeye', '3 â€“ Ironside', '4 â€“ Johnny Bravo', '5 â€“ Doublewide', '6 â€“ Machine', '7T â€“ Chain Lightning', '7T â€“ GOAT', '9T â€“ PoNY', '9T â€“ Sub Zero', '11 â€“ Ring of Fire', '12 â€“ Florida United', '13 â€“ Condors', '14 â€“ Furious George', '15 â€“ Truck Stop', '16 â€“ Madcow']\n",
      "\n",
      "Standings for 2014:\n",
      "['1 â€“ Johnny Bravo', '2 â€“ Ironside', '3T â€“ GOAT', '3T â€“ Ring of Fire', '5 â€“ Revolver', '6 â€“ Rhino', '7T â€“ Truck Stop', '7T â€“ Chain Lightning', '9T â€“ Sockeye', '9T â€“ Doublewide', '11 â€“ PoNY', '12 â€“ Temper', '13 â€“ Furious George', '14 â€“ Machine', '15 â€“ Prairie Fire', '16 â€“ Sub Zero']\n",
      "\n",
      "Standings for 2015:\n",
      "['1 â€“ Revolver', '2 â€“ Sockeye', '3T â€“ Machine', '3T â€“ Ironside', '5 â€“ Doublewide', '6 â€“ Ring of Fire', '7T â€“ Truck Stop', '7T â€“ Prairie Fire', '9T â€“ Madison Club', '9T â€“ Sub Zero', '11 â€“ Patrol', '12 â€“ GOAT', '13 â€“ High Five', '14 â€“ Johnny Bravo', '15T â€“ Rhino', '15T â€“ Florida United']\n",
      "\n",
      "Standings for 2016:\n",
      "['1 â€“ Ironside', '2 â€“ Revolver', '3T â€“ Johnny Bravo', '3T â€“ Ring of Fire', '5T â€“ Sockeye', '5T â€“ Truck Stop', '7T â€“ High Five', '7T â€“ Patrol', '9T â€“ Doublewide', '9T â€“ Furious George', '11 â€“ Prairie Fire', '12 â€“ H.I.P.', '13 â€“ Madison Club', '14 â€“ Machine', '15T â€“ Dig', '15T â€“ PoNY']\n",
      "\n",
      "Standings for 2017:\n",
      "['1 â€“ Revolver', '2 â€“ Doublewide', '3 â€“ Ring of Fire', '4 â€“ Truck Stop', '5 â€“ Ironside', '6 â€“ Dig', '7 â€“ PoNY', '8 â€“ Sub Zero', '9T â€“ Florida United', '9T â€“ Johnny Bravo', '11 â€“ Machine', '12 â€“ Sockeye', '13 â€“ High Five', '14 â€“ GOAT', '15 â€“ Condors', '16 â€“ Patrol']\n",
      "\n",
      "Standings for 2018:\n",
      "['1 â€“ PoNY', '2 â€“ Revolver', '3T â€“ Sockeye', '3T â€“ Ring of Fire', '5 â€“ Truck Stop', '6 â€“ Doublewide', '7T â€“ Sub Zero', '7T â€“ High Five', '9T â€“ Furious George', '9T â€“ Machine', '11 â€“ Johnny Bravo', '12 â€“ Temper', '13 â€“ Rhino Slam!', '14 â€“ Madison Club', '15 â€“ Chain Lightning', '16 â€“ DiG']\n",
      "\n",
      "Standings for 2019:\n",
      "['1 â€“ Sockeye', '2 â€“ Machine', '3T â€“ Ring of Fire', '3T â€“ PoNY', '5 â€“ Truck Stop', '6 â€“ Revolver', '7T â€“ Rhino Slam!', '7T â€“ Sub Zero', '9T â€“ Doublewide', '9T â€“ Furious George', '11 â€“ Condors', '12 â€“ Temper', '13 â€“ GOAT', '14 â€“ DiG', '15 â€“ Johnny Bravo', '16 â€“ Chain Lightning']\n"
     ]
    }
   ],
   "source": [
    "for url, standings in all_standings.items():\n",
    "    print(f\"\\nStandings for {url}:\")\n",
    "    print(standings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fix Pittsburgh entry in 2022 standings\n",
    "# if 2022 in all_standings:\n",
    "#     all_standings[2022] = [\n",
    "#         entry.replace('T3 â€“Pittsburgh', 'T3 â€“ Pittsburgh')  # Note the en dash (â€“) not hyphen (-)\n",
    "#         for entry in all_standings[2022]\n",
    "#     ]\n",
    "\n",
    "# # Verify the fix\n",
    "# print(\"2022 standings after fix:\")\n",
    "# for entry in all_standings[2022]:\n",
    "#     print(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty lists to store our data\n",
    "data = []\n",
    "\n",
    "# Iterate through the standings dictionary\n",
    "for year, standings in all_standings.items():\n",
    "    if isinstance(standings, list):  # Only process if we have actual standings data\n",
    "        # Counter for actual finish position\n",
    "        position_counter = 1\n",
    "        \n",
    "        for entry in standings:\n",
    "            # Split on the dash, handling both formats\n",
    "            if ' â€“ ' in entry:  # en dash\n",
    "                rank, team = entry.split(' â€“ ', 1)\n",
    "            elif ' - ' in entry:  # regular hyphen\n",
    "                rank, team = entry.split(' - ', 1)\n",
    "            else:\n",
    "                print(f\"Warning: No dash found in {entry} for year {year}\")\n",
    "                continue\n",
    "                \n",
    "            # Clean up the rank and team\n",
    "            rank = rank.strip()\n",
    "            team = team.strip()\n",
    "            \n",
    "            # Store the actual finish position\n",
    "            list_finish = position_counter\n",
    "            position_counter += 1\n",
    "            \n",
    "            data.append({\n",
    "                'Year': year,\n",
    "                'Team': team,\n",
    "                'Web_Finish': rank,  # Original format (e.g., \"T3\", \"1\")\n",
    "                'List_Finish': list_finish  # Sequential position (1, 2, 3, etc.)\n",
    "            })\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data exported to 'club-mens-raw-rankings.csv'\n"
     ]
    }
   ],
   "source": [
    "# Export to CSV\n",
    "df.to_csv('club-mens-raw-rankings.csv', index=False)\n",
    "print(\"Data exported to 'club-mens-raw-rankings.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All raw teams in alphabetical order:\n",
      "Aerodisc\n",
      "Amanata\n",
      "Anodyne\n",
      "Arm & Hammer\n",
      "BAT\n",
      "Big Ass Truck\n",
      "Big Brother\n",
      "Big River\n",
      "Blackjack\n",
      "Blaze of Glory\n",
      "Bodhi\n",
      "Bohdi\n",
      "Bomb Squad\n",
      "Bonzi\n",
      "Boost Mobile\n",
      "Booty Quake\n",
      "Boss Hogg\n",
      "Boston Ultimate\n",
      "Boulder Beer\n",
      "Boulder Stains\n",
      "COGZ\n",
      "Cbass\n",
      "Chain Lightning\n",
      "Chesapeake\n",
      "Cojones\n",
      "Commonwealth\n",
      "Condors\n",
      "Cornell\n",
      "Death or Glory\n",
      "DiG\n",
      "Dig\n",
      "Double Happiness\n",
      "Doublewide\n",
      "Drivers\n",
      "Earth Atomizer\n",
      "East Bay\n",
      "El Diablo\n",
      "Electric Pig\n",
      "FBI\n",
      "Fat Women\n",
      "First Time Gary\n",
      "Florida\n",
      "Florida United\n",
      "Flying Circus\n",
      "Fun Hogs\n",
      "Furious George\n",
      "G-Unit\n",
      "GOAT\n",
      "Gang\n",
      "Glassboro\n",
      "Graffiti\n",
      "Grafitti\n",
      "H.I.P.\n",
      "High Five\n",
      "Hostages\n",
      "Houndz\n",
      "Huckin' Foosiers\n",
      "Iguanas\n",
      "Ironside\n",
      "JAM\n",
      "Jaga\n",
      "Jam\n",
      "Johnny Bravo\n",
      "Kaboom\n",
      "Kaos\n",
      "Knights of Nee\n",
      "Lemon\n",
      "Looney Tunes\n",
      "Los Guapos\n",
      "Machine\n",
      "Madcow\n",
      "Madison\n",
      "Madison Club\n",
      "Metal\n",
      "Miami\n",
      "Miami Refugees\n",
      "Michigan State\n",
      "Monster\n",
      "Mr. Bubble\n",
      "Mr. Pouce\n",
      "New York\n",
      "Nice Guys\n",
      "Night Train\n",
      "Oakland\n",
      "Oregon Donors\n",
      "Orlando\n",
      "PBR Streetgang\n",
      "Parking Lot JAM\n",
      "Patrol\n",
      "Philmore\n",
      "Pike\n",
      "PoNY\n",
      "Polo Club\n",
      "Port City Slickers\n",
      "Potomac\n",
      "Prairie Fire\n",
      "Pump House 5\n",
      "R & B\n",
      "Rage\n",
      "Randall's Island\n",
      "Red Tide\n",
      "Refugees\n",
      "Revolver\n",
      "Rhino\n",
      "Rhino Slam!\n",
      "Ring of Fire\n",
      "Rivals\n",
      "RoQ\n",
      "Rude Boys\n",
      "San Diego\n",
      "Saucy Jack\n",
      "Second Wind\n",
      "Secret Squirrel\n",
      "Sky Pilots\n",
      "Snapple\n",
      "Sockeye\n",
      "South Bay\n",
      "South Coast\n",
      "Southpaw\n",
      "Spinoffs\n",
      "Splat\n",
      "Static Disc\n",
      "Streetgang\n",
      "Sub Zero\n",
      "Tanasi\n",
      "Team \"O\"\n",
      "Temper\n",
      "Texas Heat\n",
      "The Gang\n",
      "The Van Buren Boys\n",
      "Titanic\n",
      "Trouble Club\n",
      "Truck Stop\n",
      "Tsunami\n",
      "Tunas\n",
      "Vicious Cycle\n",
      "WSL\n",
      "WUDI\n",
      "Warriors\n",
      "Windy City\n",
      "Yo Mama\n",
      "Z\n"
     ]
    }
   ],
   "source": [
    "# Get unique team names and sort alphabetically\n",
    "unique_teams_raw = sorted(df['Team'].unique())\n",
    "\n",
    "# Print the teams\n",
    "print(\"All raw teams in alphabetical order:\")\n",
    "for team in unique_teams_raw:\n",
    "    print(team)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_team_name(name):\n",
    "    # First, strip whitespace and convert to title case\n",
    "    name = name.strip().title()\n",
    "    \n",
    "    # Standardize team names\n",
    "    replacements = {\n",
    "        \"Bohdi\": \"Bodhi\",\n",
    "        \"Dig\": \"DiG\",\n",
    "        \"Grafitti\": \"Graffiti\",\n",
    "        \"Jam\": \"JAM\",\n",
    "        \"Gang\": \"The Gang\",\n",
    "        \"Miami Refugees\": \"Refugees\",\n",
    "        \"PBR Streetgang\": \"Streetgang\",\n",
    "        \"Rhino\": \"Rhino Slam!\",\n",
    "        \"BAT\": \"Big Ass Truck\",\n",
    "    }\n",
    "    \n",
    "    for old, new in replacements.items():\n",
    "        if name.upper() == old.upper():  # Case-insensitive replacement\n",
    "            name = new\n",
    "    \n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All teams after cleaning (alphabetically):\n",
      "Aerodisc\n",
      "Amanata\n",
      "Anodyne\n",
      "Arm & Hammer\n",
      "Big Ass Truck\n",
      "Big Brother\n",
      "Big River\n",
      "Blackjack\n",
      "Blaze Of Glory\n",
      "Bodhi\n",
      "Bomb Squad\n",
      "Bonzi\n",
      "Boost Mobile\n",
      "Booty Quake\n",
      "Boss Hogg\n",
      "Boston Ultimate\n",
      "Boulder Beer\n",
      "Boulder Stains\n",
      "Cbass\n",
      "Chain Lightning\n",
      "Chesapeake\n",
      "Cogz\n",
      "Cojones\n",
      "Commonwealth\n",
      "Condors\n",
      "Cornell\n",
      "Death Or Glory\n",
      "DiG\n",
      "Double Happiness\n",
      "Doublewide\n",
      "Drivers\n",
      "Earth Atomizer\n",
      "East Bay\n",
      "El Diablo\n",
      "Electric Pig\n",
      "Fat Women\n",
      "Fbi\n",
      "First Time Gary\n",
      "Florida\n",
      "Florida United\n",
      "Flying Circus\n",
      "Fun Hogs\n",
      "Furious George\n",
      "G-Unit\n",
      "Glassboro\n",
      "Goat\n",
      "Graffiti\n",
      "H.I.P.\n",
      "High Five\n",
      "Hostages\n",
      "Houndz\n",
      "Huckin' Foosiers\n",
      "Iguanas\n",
      "Ironside\n",
      "JAM\n",
      "Jaga\n",
      "Johnny Bravo\n",
      "Kaboom\n",
      "Kaos\n",
      "Knights Of Nee\n",
      "Lemon\n",
      "Looney Tunes\n",
      "Los Guapos\n",
      "Machine\n",
      "Madcow\n",
      "Madison\n",
      "Madison Club\n",
      "Metal\n",
      "Miami\n",
      "Michigan State\n",
      "Monster\n",
      "Mr. Bubble\n",
      "Mr. Pouce\n",
      "New York\n",
      "Nice Guys\n",
      "Night Train\n",
      "Oakland\n",
      "Oregon Donors\n",
      "Orlando\n",
      "Parking Lot Jam\n",
      "Patrol\n",
      "Philmore\n",
      "Pike\n",
      "Polo Club\n",
      "Pony\n",
      "Port City Slickers\n",
      "Potomac\n",
      "Prairie Fire\n",
      "Pump House 5\n",
      "R & B\n",
      "Rage\n",
      "Randall'S Island\n",
      "Red Tide\n",
      "Refugees\n",
      "Revolver\n",
      "Rhino Slam!\n",
      "Ring Of Fire\n",
      "Rivals\n",
      "Roq\n",
      "Rude Boys\n",
      "San Diego\n",
      "Saucy Jack\n",
      "Second Wind\n",
      "Secret Squirrel\n",
      "Sky Pilots\n",
      "Snapple\n",
      "Sockeye\n",
      "South Bay\n",
      "South Coast\n",
      "Southpaw\n",
      "Spinoffs\n",
      "Splat\n",
      "Static Disc\n",
      "Streetgang\n",
      "Sub Zero\n",
      "Tanasi\n",
      "Team \"O\"\n",
      "Temper\n",
      "Texas Heat\n",
      "The Gang\n",
      "The Van Buren Boys\n",
      "Titanic\n",
      "Trouble Club\n",
      "Truck Stop\n",
      "Tsunami\n",
      "Tunas\n",
      "Vicious Cycle\n",
      "Warriors\n",
      "Windy City\n",
      "Wsl\n",
      "Wudi\n",
      "Yo Mama\n",
      "Z\n"
     ]
    }
   ],
   "source": [
    "# Apply the cleaning function to create new column\n",
    "df['Team_Clean'] = df['Team'].apply(clean_team_name)\n",
    "\n",
    "# Print unique teams after cleaning to verify changes\n",
    "print(\"All teams after cleaning (alphabetically):\")\n",
    "for team in sorted(df['Team_Clean'].unique()):\n",
    "    print(team)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Team</th>\n",
       "      <th>Web_Finish</th>\n",
       "      <th>List_Finish</th>\n",
       "      <th>Team_Clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1979</td>\n",
       "      <td>Glassboro</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Glassboro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1979</td>\n",
       "      <td>Condors</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Condors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1979</td>\n",
       "      <td>Michigan State</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Michigan State</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1979</td>\n",
       "      <td>Orlando</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Orlando</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1979</td>\n",
       "      <td>Cornell</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>Cornell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>2019</td>\n",
       "      <td>Temper</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>Temper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>2019</td>\n",
       "      <td>GOAT</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>Goat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>2019</td>\n",
       "      <td>DiG</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>DiG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>2019</td>\n",
       "      <td>Johnny Bravo</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>Johnny Bravo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>2019</td>\n",
       "      <td>Chain Lightning</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>Chain Lightning</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>551 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Year             Team Web_Finish  List_Finish       Team_Clean\n",
       "0    1979        Glassboro          1            1        Glassboro\n",
       "1    1979          Condors          2            2          Condors\n",
       "2    1979   Michigan State          3            3   Michigan State\n",
       "3    1979          Orlando          4            4          Orlando\n",
       "4    1979          Cornell          5            5          Cornell\n",
       "..    ...              ...        ...          ...              ...\n",
       "546  2019           Temper         12           12           Temper\n",
       "547  2019             GOAT         13           13             Goat\n",
       "548  2019              DiG         14           14              DiG\n",
       "549  2019     Johnny Bravo         15           15     Johnny Bravo\n",
       "550  2019  Chain Lightning         16           16  Chain Lightning\n",
       "\n",
       "[551 rows x 5 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_rank_format(df, column_name):\n",
    "    def convert_format(rank):\n",
    "        # If it's already in T# format, keep it\n",
    "        if isinstance(rank, str) and rank.startswith('T'):\n",
    "            return rank\n",
    "        \n",
    "        # If it ends with T, convert to T# format\n",
    "        if isinstance(rank, str) and rank.endswith('T'):\n",
    "            number = rank[:-1]  # Remove the T\n",
    "            return f'T{number}'\n",
    "        \n",
    "        return rank  # Return unchanged if no T\n",
    "\n",
    "    # Apply the conversion to the specified column\n",
    "    df[column_name] = df[column_name].apply(convert_format)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data exported to 'club-mens-rankings.csv'\n"
     ]
    }
   ],
   "source": [
    "# Create new dataframe without URL and Team columns\n",
    "df_T = df.drop(['Team'], axis=1)\n",
    "\n",
    "# Apply the conversion to the specified column\n",
    "df = transform_rank_format(df_T, 'Web_Finish')\n",
    "\n",
    "# Rename Team_Clean to Team for clarity\n",
    "df_T = df_T.rename(columns={'Team_Clean': 'Team', 'List_Finish': 'Rank', 'Web_Finish': 'T_Rank'})\n",
    "\n",
    "# Reorder columns\n",
    "df_T = df_T[['Team', 'Year', 'Rank', 'T_Rank']]\n",
    "\n",
    "# Manually change a rank value for UNC Wilmington in 2021\n",
    "# df_T.loc[(df_T['Team'] == 'UNC Wilmington') & (df_T['Year'] == 2021), 'T_Rank'] = 'T9'\n",
    "\n",
    "# Verify the first few rows to check the mapping\n",
    "df_T\n",
    "\n",
    "# Export to CSV\n",
    "df_T.to_csv('club-mens-rankings.csv', index=False)\n",
    "print(\"Data exported to 'club-mens-rankings.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read the CSV\n",
    "# df_with_less_columns_for_tableau = pd.read_csv('ultimate_standings_clean.csv')\n",
    "\n",
    "# # Group by year and check rank sequences\n",
    "# found_issues = False\n",
    "# for year in sorted(df_with_less_columns_for_tableau['Year'].unique()):\n",
    "#     year_data = df_with_less_columns_for_tableau[df_with_less_columns_for_tableau['Year'] == year].sort_values('Rank')\n",
    "#     ranks = year_data['Rank'].tolist()\n",
    "#     expected_ranks = list(range(1, len(ranks) + 1))\n",
    "    \n",
    "#     if ranks != expected_ranks:\n",
    "#         found_issues = True\n",
    "#         print(f\"\\nYear {year} has non-sequential ranks:\")\n",
    "#         print(\"Expected:\", expected_ranks)\n",
    "#         print(\"Actual:\", ranks)\n",
    "#         print(\"\\nFull data for this year:\")\n",
    "#         print(year_data[['Team', 'Rank']].to_string())\n",
    "\n",
    "# if not found_issues:\n",
    "#     print(\"All years have sequential ranks without gaps! âœ“\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Web_Finish</th>\n",
       "      <th>List_Finish</th>\n",
       "      <th>Team_Clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1979</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Glassboro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1979</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Condors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1979</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Michigan State</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1979</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Orlando</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1979</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>Cornell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>2019</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>Temper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>2019</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>Goat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>2019</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>DiG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>2019</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>Johnny Bravo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>2019</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>Chain Lightning</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>551 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Year Web_Finish  List_Finish       Team_Clean\n",
       "0    1979          1            1        Glassboro\n",
       "1    1979          2            2          Condors\n",
       "2    1979          3            3   Michigan State\n",
       "3    1979          4            4          Orlando\n",
       "4    1979          5            5          Cornell\n",
       "..    ...        ...          ...              ...\n",
       "546  2019         12           12           Temper\n",
       "547  2019         13           13             Goat\n",
       "548  2019         14           14              DiG\n",
       "549  2019         15           15     Johnny Bravo\n",
       "550  2019         16           16  Chain Lightning\n",
       "\n",
       "[551 rows x 4 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the existing CSV\n",
    "df_T = pd.read_csv('club-mens-rankings.csv')\n",
    "\n",
    "# Remove any existing 2021-2024 entries\n",
    "df_T = df_T[~df_T['Year'].isin([2021, 2022, 2023, 2024])]\n",
    "\n",
    "# Create 2021 data\n",
    "data_2021 = {\n",
    "    'Team': [\n",
    "        'Ring of Fire', 'PoNY', 'Sockeye', 'Rhino Slam!', 'Machine',\n",
    "        'Truck Stop', 'DiG', 'Temper', 'Johnny Bravo', 'Chain Lightning',\n",
    "        'Condors', 'Sprout', 'Revolver', 'Sub Zero', 'Killjoys',\n",
    "        'Lotus'\n",
    "    ],\n",
    "    'Year': [2021] * 16,\n",
    "    'Rank': list(range(1, 17)),\n",
    "    'T_Rank': [\n",
    "        '1', '2', 'T3', 'T3', '5', \n",
    "        '6', 'T7', 'T7', 'T9', 'T9',\n",
    "        '11', '12', '13', '14', '15',\n",
    "        '16'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create 2022 data\n",
    "data_2022 = {\n",
    "    'Team': [\n",
    "        'Johnny Bravo', 'Truck Stop', 'PoNY', 'Rhino Slam!', 'Machine',\n",
    "        'Ring of Fire', 'Doublewide', 'Revolver', 'Vault', 'Chain Lightning',\n",
    "        'Temper', 'Omen', 'GOAT', 'Sockeye', 'Mad Men',\n",
    "        'Condors'\n",
    "    ],\n",
    "    'Year': [2022] * 16,\n",
    "    'Rank': list(range(1, 17)),\n",
    "    'T_Rank': [\n",
    "        '1', '2', 'T3', 'T3', '5', \n",
    "        '6', 'T7', 'T7', 'T9', 'T9',\n",
    "        '11', '12', '13', '14', '15',\n",
    "        '16'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create 2023 data\n",
    "data_2023 = {\n",
    "    'Team': [\n",
    "        'Truck Stop', 'Machine', 'Johnny Bravo', 'Ring of Fire', 'DiG',\n",
    "        'PoNY', 'Rhino Slam!', 'Revolver', 'Furious George', 'Chain Lightning',\n",
    "        'Doublewide', 'Raleigh-Durham United', 'Sub Zero', 'Vault', 'Dark Star',\n",
    "        'Blueprint'\n",
    "    ],\n",
    "    'Year': [2023] * 16,\n",
    "    'Rank': list(range(1, 17)),\n",
    "    'T_Rank': [\n",
    "        '1', '2', 'T3', 'T3', '5', \n",
    "        '6', 'T7', 'T7', 'T9', 'T9',\n",
    "        '11', '12', '13', '14', '15',\n",
    "        '16'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create 2024 data\n",
    "data_2024 = {\n",
    "    'Team': [\n",
    "        'Rhino Slam!', 'PoNY', 'Revolver', 'Machine', 'DiG',\n",
    "        'Truck Stop', 'Ring of Fire', 'Johnny Bravo', 'Chain Lightning', 'Furious George',\n",
    "        'Doublewide', 'Mallard', 'Sockeye', 'Raleigh-Durham United', 'GOAT',\n",
    "        'Shrimp'\n",
    "    ],\n",
    "    'Year': [2024] * 16,\n",
    "    'Rank': list(range(1, 17)),\n",
    "    'T_Rank': [\n",
    "        '1', '2', 'T3', 'T3', '5', \n",
    "        '6', 'T7', 'T7', 'T9', 'T9',\n",
    "        '11', '12', '13', '14', '15',\n",
    "        '16'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Convert to DataFrames\n",
    "new_df_2021 = pd.DataFrame(data_2021)\n",
    "new_df_2022 = pd.DataFrame(data_2022)\n",
    "new_df_2023 = pd.DataFrame(data_2023)\n",
    "new_df_2024 = pd.DataFrame(data_2024)\n",
    "\n",
    "# Append to existing DataFrame\n",
    "df_T = pd.concat([df_T, new_df_2021, new_df_2022, new_df_2023, new_df_2024], ignore_index=True)\n",
    "\n",
    "# Export to CSV\n",
    "df_T.to_csv('club-mens-rankings.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
