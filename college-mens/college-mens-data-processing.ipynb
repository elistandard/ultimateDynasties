{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup(url, delay=1, headers=None, proxy=None):\n",
    "    \"\"\"\n",
    "    Fetches and parses a webpage with BeautifulSoup, handling 403 errors\n",
    "    by setting headers, delays, and optional proxies.\n",
    "\n",
    "    Parameters:\n",
    "        url (str): The URL of the webpage to scrape.\n",
    "        delay (int): The delay (in seconds) between requests. Default is 1 second.\n",
    "        headers (dict): Optional headers to include in the request.\n",
    "        proxy (dict): Optional dictionary for proxies.\n",
    "\n",
    "    Returns:\n",
    "        BeautifulSoup object of the parsed page or None if request fails.\n",
    "    \"\"\"\n",
    "\n",
    "    # Default headers to mimic a browser request if none are provided\n",
    "    if headers is None:\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "        }\n",
    "\n",
    "    try:\n",
    "        # Send the request with headers and optional proxy\n",
    "        response = requests.get(url, headers=headers, proxies=proxy)\n",
    "        \n",
    "        # Check for 403 Forbidden error\n",
    "        if response.status_code == 403:\n",
    "            print(\"403 Forbidden: Access to the page is restricted.\")\n",
    "            return None\n",
    "        \n",
    "        # Add delay to avoid rapid requests\n",
    "        time.sleep(delay)\n",
    "\n",
    "        # Parse the content with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        return soup\n",
    "    \n",
    "    except requests.RequestException as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soup = get_soup(URL, delay=2)\n",
    "# if soup:\n",
    "#     print(soup.prettify())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_list(url):\n",
    "    try:\n",
    "        # Set headers to mimic a browser request\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "        }\n",
    "        \n",
    "        # Send a GET request to the URL\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()  # Raise an error for bad responses\n",
    "\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find the list within the specified section and div\n",
    "        standings_div = soup.find('div', class_='standings')\n",
    "        if standings_div:\n",
    "            list_items = standings_div.find_all('li')\n",
    "            standings = [item.text.strip() for item in list_items if item.text.strip()]\n",
    "            return standings\n",
    "        else:\n",
    "            print(f\"No standings found for {url}\")\n",
    "            return None\n",
    "\n",
    "    except requests.HTTPError as http_err:\n",
    "        print(f\"HTTP error occurred while scraping {url}: {http_err}\")\n",
    "        return None\n",
    "    except requests.RequestException as req_err:\n",
    "        print(f\"Request error occurred while scraping {url}: {req_err}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while scraping {url}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def scrape_multiple_urls(urls):\n",
    "#     # Dictionary to store the results for each URL\n",
    "#     results = {}\n",
    "#     for url in urls:\n",
    "#         print(f\"Scraping data from {url}...\")\n",
    "#         data = scrape_list(url)\n",
    "#         results[url] = data if data else \"No data found\"\n",
    "#     return results\n",
    "\n",
    "def scrape_multiple_urls_dict(urls):\n",
    "    # Dictionary to store the results for each year\n",
    "    results = {}\n",
    "    for year, url in urls.items():\n",
    "        print(f\"Scraping data for the year {year} from {url}...\")\n",
    "        data = scrape_list(url)\n",
    "        results[year] = data if data else \"No data found\"\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = {\n",
    "    1984: 'https://collegechampionships.usaultimate.org/d1-men/history/1984-d-i-men/',\n",
    "    1985: 'https://collegechampionships.usaultimate.org/d1-men/history/1985-d-i-men/',\n",
    "    1986: 'https://collegechampionships.usaultimate.org/d1-men/history/1986-d-i-men/',\n",
    "    1987: 'https://collegechampionships.usaultimate.org/d1-men/history/1987-d-i-men/',\n",
    "    1988: 'https://collegechampionships.usaultimate.org/d1-men/history/1988-d-i-men/',\n",
    "    1989: 'https://collegechampionships.usaultimate.org/d1-men/history/1989-d-i-men/',\n",
    "    1990: 'https://collegechampionships.usaultimate.org/d1-men/history/1990-d-i-men/',\n",
    "    1991: 'https://collegechampionships.usaultimate.org/d1-men/history/1991-d-i-men/',\n",
    "    1992: 'https://collegechampionships.usaultimate.org/d1-men/history/1992-d-i-men/',\n",
    "    1993: 'https://collegechampionships.usaultimate.org/d1-men/history/1993-d-i-men/',\n",
    "    1994: 'https://collegechampionships.usaultimate.org/d1-men/history/1994-d-i-men/',\n",
    "    1995: 'https://collegechampionships.usaultimate.org/d1-men/history/1995-d-i-men/',\n",
    "    1996: 'https://collegechampionships.usaultimate.org/d1-men/history/1996-d-i-men/',\n",
    "    1997: 'https://collegechampionships.usaultimate.org/d1-men/history/1997-d-i-men/',\n",
    "    1998: 'https://collegechampionships.usaultimate.org/d1-men/history/1998-d-i-men/',\n",
    "    1999: 'https://collegechampionships.usaultimate.org/d1-men/history/1999-d-i-men/',\n",
    "    2000: 'https://collegechampionships.usaultimate.org/d1-men/history/2000-d-i-men/',\n",
    "    2001: 'https://collegechampionships.usaultimate.org/d1-men/history/2001-d-i-men/',\n",
    "    2002: 'https://collegechampionships.usaultimate.org/d1-men/history/2002-d-i-men/',\n",
    "    2003: 'https://collegechampionships.usaultimate.org/d1-men/history/2003-d-i-men/',\n",
    "    2004: 'https://collegechampionships.usaultimate.org/d1-men/history/2004-d-i-men/',\n",
    "    2005: 'https://collegechampionships.usaultimate.org/d1-men/history/2005-d-i-men/',\n",
    "    2006: 'https://collegechampionships.usaultimate.org/d1-men/history/2006-d-i-men/',\n",
    "    2007: 'https://collegechampionships.usaultimate.org/d1-men/history/2007-d-i-men/',\n",
    "    2008: 'https://collegechampionships.usaultimate.org/d1-men/history/2008-d-i-men/',\n",
    "    2009: 'https://collegechampionships.usaultimate.org/d1-men/history/2009-di-men/',\n",
    "    2010: 'https://collegechampionships.usaultimate.org/d1-men/history/2010-d1-men/',\n",
    "    2011: 'https://collegechampionships.usaultimate.org/d1-men/history/2011-d1-men/',\n",
    "    2012: 'https://collegechampionships.usaultimate.org/d1-men/history/2012-d1-men/',\n",
    "    2013: 'https://collegechampionships.usaultimate.org/d1-men/history/2013-d1-men/',\n",
    "    2014: 'https://collegechampionships.usaultimate.org/d1-men/history/2014-d1-men/',\n",
    "    2015: 'https://collegechampionships.usaultimate.org/d1-men/history/2015-d1-men/',\n",
    "    2016: 'https://collegechampionships.usaultimate.org/d1-men/history/2016-d1-men/',\n",
    "    2017: 'https://collegechampionships.usaultimate.org/d1-men/history/2017-d1-men/',\n",
    "    2018: 'https://collegechampionships.usaultimate.org/d1-men/history/2018-d-i-men/',\n",
    "    2019: 'https://collegechampionships.usaultimate.org/d1-men/history/2019-d-i-men/',\n",
    "    # 2020 no season\n",
    "    2021: 'https://collegechampionships.usaultimate.org/d1-men/history/2019-d-i-men-2/',\n",
    "    2022: 'https://collegechampionships.usaultimate.org/d1-men/history/2019-d-i-men-2-2/',\n",
    "    2023: 'https://collegechampionships.usaultimate.org/d1-men/history/2019-d-i-men-2-2-2/',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for the year 1984 from https://collegechampionships.usaultimate.org/d1-men/history/1984-d-i-men/...\n",
      "Scraping data for the year 1985 from https://collegechampionships.usaultimate.org/d1-men/history/1985-d-i-men/...\n",
      "Scraping data for the year 1986 from https://collegechampionships.usaultimate.org/d1-men/history/1986-d-i-men/...\n",
      "Scraping data for the year 1987 from https://collegechampionships.usaultimate.org/d1-men/history/1987-d-i-men/...\n",
      "Scraping data for the year 1988 from https://collegechampionships.usaultimate.org/d1-men/history/1988-d-i-men/...\n",
      "Scraping data for the year 1989 from https://collegechampionships.usaultimate.org/d1-men/history/1989-d-i-men/...\n",
      "Scraping data for the year 1990 from https://collegechampionships.usaultimate.org/d1-men/history/1990-d-i-men/...\n",
      "Scraping data for the year 1991 from https://collegechampionships.usaultimate.org/d1-men/history/1991-d-i-men/...\n",
      "Scraping data for the year 1992 from https://collegechampionships.usaultimate.org/d1-men/history/1992-d-i-men/...\n",
      "Scraping data for the year 1993 from https://collegechampionships.usaultimate.org/d1-men/history/1993-d-i-men/...\n",
      "Scraping data for the year 1994 from https://collegechampionships.usaultimate.org/d1-men/history/1994-d-i-men/...\n",
      "Scraping data for the year 1995 from https://collegechampionships.usaultimate.org/d1-men/history/1995-d-i-men/...\n",
      "Scraping data for the year 1996 from https://collegechampionships.usaultimate.org/d1-men/history/1996-d-i-men/...\n",
      "Scraping data for the year 1997 from https://collegechampionships.usaultimate.org/d1-men/history/1997-d-i-men/...\n",
      "Scraping data for the year 1998 from https://collegechampionships.usaultimate.org/d1-men/history/1998-d-i-men/...\n",
      "Scraping data for the year 1999 from https://collegechampionships.usaultimate.org/d1-men/history/1999-d-i-men/...\n",
      "Scraping data for the year 2000 from https://collegechampionships.usaultimate.org/d1-men/history/2000-d-i-men/...\n",
      "Scraping data for the year 2001 from https://collegechampionships.usaultimate.org/d1-men/history/2001-d-i-men/...\n",
      "Scraping data for the year 2002 from https://collegechampionships.usaultimate.org/d1-men/history/2002-d-i-men/...\n",
      "Scraping data for the year 2003 from https://collegechampionships.usaultimate.org/d1-men/history/2003-d-i-men/...\n",
      "Scraping data for the year 2004 from https://collegechampionships.usaultimate.org/d1-men/history/2004-d-i-men/...\n",
      "Scraping data for the year 2005 from https://collegechampionships.usaultimate.org/d1-men/history/2005-d-i-men/...\n",
      "Scraping data for the year 2006 from https://collegechampionships.usaultimate.org/d1-men/history/2006-d-i-men/...\n",
      "Scraping data for the year 2007 from https://collegechampionships.usaultimate.org/d1-men/history/2007-d-i-men/...\n",
      "Scraping data for the year 2008 from https://collegechampionships.usaultimate.org/d1-men/history/2008-d-i-men/...\n",
      "Scraping data for the year 2009 from https://collegechampionships.usaultimate.org/d1-men/history/2009-di-men/...\n",
      "Scraping data for the year 2010 from https://collegechampionships.usaultimate.org/d1-men/history/2010-d1-men/...\n",
      "Scraping data for the year 2011 from https://collegechampionships.usaultimate.org/d1-men/history/2011-d1-men/...\n",
      "Scraping data for the year 2012 from https://collegechampionships.usaultimate.org/d1-men/history/2012-d1-men/...\n",
      "Scraping data for the year 2013 from https://collegechampionships.usaultimate.org/d1-men/history/2013-d1-men/...\n",
      "Scraping data for the year 2014 from https://collegechampionships.usaultimate.org/d1-men/history/2014-d1-men/...\n",
      "Scraping data for the year 2015 from https://collegechampionships.usaultimate.org/d1-men/history/2015-d1-men/...\n",
      "Scraping data for the year 2016 from https://collegechampionships.usaultimate.org/d1-men/history/2016-d1-men/...\n",
      "Scraping data for the year 2017 from https://collegechampionships.usaultimate.org/d1-men/history/2017-d1-men/...\n",
      "Scraping data for the year 2018 from https://collegechampionships.usaultimate.org/d1-men/history/2018-d-i-men/...\n",
      "Scraping data for the year 2019 from https://collegechampionships.usaultimate.org/d1-men/history/2019-d-i-men/...\n",
      "Scraping data for the year 2021 from https://collegechampionships.usaultimate.org/d1-men/history/2019-d-i-men-2/...\n",
      "Scraping data for the year 2022 from https://collegechampionships.usaultimate.org/d1-men/history/2019-d-i-men-2-2/...\n",
      "Scraping data for the year 2023 from https://collegechampionships.usaultimate.org/d1-men/history/2019-d-i-men-2-2-2/...\n"
     ]
    }
   ],
   "source": [
    "all_standings = scrape_multiple_urls_dict(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Standings for 1984:\n",
      "['1- Stanford', '2-Glassboro', '3T-Massachusetts', '3T-Pennsylvania', '?-Chabot Community College', '?-Kansas', '?-Ohio', '?-Syracuse', '?-Texas', '?-Tufts']\n",
      "\n",
      "Standings for 1985:\n",
      "['1-Pennsylvania', '2-Massachusetts', '3T-Cornell', '3T-SW Missouri State', '5T-Stanford', '5T-Texas', '7T-Cal Poly SLO', '7T-Kansas', '9T-MIT', '9T-Oregon', '11T-Central Florida', '11T-Princeton']\n",
      "\n",
      "Standings for 1986:\n",
      "['1-Massachusetts', '2-Stanford', '3T-California-Santa Barbara', '3T-Cornell', '5T-Oregon', '5T-SW Missouri State', '7T-Kansas', '7T-MIT', '9T-Carnegie Mellon', '9T-Princeton', '11T-Georgia', '11T-Texas']\n",
      "\n",
      "Standings for 1987:\n",
      "['1-Chabot Community College', '2-California-Santa Barbara', '3T-Cal Poly SLO', '3T-Cornell', '5T-SW Missouri State', '5T-Texas', '7T-Georgia Tech', '7T-SUNY-Purchase', '9T-Carnegie Mellon', '9T-Princeton', '11T-Kansas', '11T-Michigan', '13T-East Carolina', '13T-Vermont']\n",
      "\n",
      "Standings for 1988:\n",
      "['1-California-Santa Barbara', '2-Texas', '3-Stanford', '4-Columbia', '5T-Georgia Tech', '5T-Kansas', '7T-Carnegie Mellon', '7T-East Carolina', '9T-Saint Louis', '9T-Wesleyan', '11T-Massachusetts', '11T-Winona State']\n",
      "\n",
      "Standings for 1989:\n",
      "['1-California-Santa Barbara', '2-Stanford', '3T-Carnegie Mellon', '3T-Texas', '?-Columbia', '?-East Carolina', '?-Florida', '?-Indiana', '?-Kansas', '?-Pennsylvania', '?-SUNY-Purchase', '?-Tufts']\n",
      "\n",
      "Standings for 1990:\n",
      "['1-California-Santa Barbara', '2-North Carolina-Wilmington', '3-SUNY-Purchase', '4-Cornell', '5-Vermont', '6-California-Santa Cruz', '7-Wisconsin', '8-Carleton College', '9T-Georgia', '9T-Kansas', '9T-Princeton', '9T-Texas']\n",
      "\n",
      "Standings for 1991:\n",
      "['1-California-Santa Cruz', '2-North Carolina-Wilmington', '3-California-Santa Barbara', '4-Cornell', '5-Pennsylvania', '6-Carleton', '7T-Boston College', '7T-Georgia', '7T-Wisconsin', '10-East Carolina', '11T-Georgia Tech', '11T-Kansas']\n",
      "\n",
      "Standings for 1992:\n",
      "['1-Oregon', '2-Cornell', '3-North Carolina-Wilmington', '4-California', '?-Carleton College', '?-East Carolina', '?-Georgia Tech', '?-Oberlin', '?-Texas', '?-Vermont', '?-Wesleyan', '?-Wisconsin']\n",
      "\n",
      "Standings for 1993:\n",
      "['1-North Carolina-Wilmington', '2-California-Santa Barbara', '3T-California-Santa Cruz', '3T-Carleton College', '5T-East Carolina', '5T-Texas', '7T-Georgia', '7T-Wisconsin', '9T-Boston College', '9T-Kansas', '11T-SUNY-Albany', '11T-Wesleyan']\n",
      "\n",
      "Standings for 1994:\n",
      "['1- East Carolina', '2-Stanford', '3T-California-Santa Barbara', '3T-Carleton College', '5T-Georgia', '5T-Wisconsin', '7T-Cornell', '7T- Las Positas', '9T-Kansas', '9T-Pennsylvania', '11T-SUNY-Binghampton', '11T-Texas']\n",
      "\n",
      "Standings for 1995:\n",
      "['1- East Carolina', '2-California-Santa Cruz', '3T-North Carolina- Wilmington', '3T-Stanford', '5T-Cornell', '5T-Yale', '7T-Carleton College', '7T-LSU', '9T-Oberlin', '9T-Williams', '11T-Georgia', '11T-Indiana']\n",
      "\n",
      "Standings for 1996:\n",
      "['1- California-Santa Barbara', '2-Carleton College', '3T-Cornell', '3T-Wisconsin', '5T-North Carolina-Wilmington', '5T-Stanford', '7T-Georgia', '7T-North Carolina State', '9T-Wesleyan', '9T-Williams', '11T-Indiana', '11T-Florida State']\n",
      "\n",
      "Standings for 1997:\n",
      "['1- California- Santa Barbara', '2- Stanford', '3T-Carleton College', '3T-East Carolina', '5T-North Carolina- Wilmington', '5T-Wisconsin', '7T-Cornell', '7T-Rice', '9T-Oberlin', '9T-Wesleyan', '11T-LSU', '11T-Yale']\n",
      "\n",
      "Standings for 1998:\n",
      "['1- California-Santa Barbara', '2-Stanford', '3T-Brown', '3T-North Carolina', '5T-Carleton College', '5T-Colorado', '7T-Harvard', '7T-LSU', '9T-East Carolina', '9T-Iowa', '11T-Rice', '11T-Yale']\n",
      "\n",
      "Standings for 1999:\n",
      "['1- North Carolina State', '2-California-Santa Barbara', '3T-Brown', '3T-Carleton College', '5T-California- Santa Cruz', '5T-Colorado', '5T-North Carolina', '5T-Stanford', '9T-Cornell', '9T-Iowa', '9T-Michigan', '9T-Yale', '13T-Kansas', '13T-Oberlin', '13T-Rice', '13T-Salisbury']\n",
      "\n",
      "Standings for 2000:\n",
      "['1- Brown', '2-Carleton College', '3T-California- Santa Barbara', '3T-Colorado', '5T-North Carolina', '5T-North Carolina State', '5T-Salisbury', '5T-Stanford', '9-Michigan', '10-Wisconsin', '11T-Rice', '11T-Winona State', '13-Notre Dame', '14-Princeton', '15T-California -Santa Cruz', '15T-Tufts']\n",
      "\n",
      "Standings for 2001:\n",
      "['1- Carleton College', '2- Colorado', '3-California-Santa Barbara', '4-Oregon', '5T-Brown', '5T- Cornell', '5T-Michigan', '5T-Wisconsin', '9-Harvard', '10-Pennsylvania', '11-Ohio State', '12-Texas A&M', '13-Duke', '14-North Carolina-Wilmington', '15-North Carolina', '16-Tufts']\n",
      "\n",
      "Standings for 2002:\n",
      "['1- Stanford', '2-Wisconsin', '3T-Carleton College', '3T-William and Mary', '5T-California- Santa Barbara', '5T-Michigan', '5T-North Carolina- Wilmington', '5T-Tufts', '9T-Cornell', '9T-Illinois', '11T-California- Santa Cruz', '11T-Colorado', '13T-Iowa', '13T-Swarthmore', '15T-George Washington', '15T-Texas']\n",
      "\n",
      "Standings for 2003:\n",
      "['1-Wisconsin', '2-Oregon', '3T-Carleton College', '3T-Colorado', '5T-Brown', '5T-Texas', '5T-William and Mary', '5T-Illinois', '9T-Georgia', '9T-Michigan', '11T-North Carolina State', '11T-Penn State', '13- Ohio State', '14-Pennsylvania', '15-Dartmouth', '16-Williams']\n",
      "\n",
      "Standings for 2004:\n",
      "['1- Colorado', '2-California', '3T-Brown', '3T-Stanford', '5T-California-San Diego', '5T-Iowa', '5T-North Carolina State', '5T-Wisconsin', '9T-Carleton College', '9T-Tufts', '11T-George Washington', '11T-William and Mary', '13- Michigan State', '14-Illinois', '15-Delaware', '16-Kansas']\n",
      "\n",
      "Standings for 2005:\n",
      "['1- Brown', '2- Colorado', '3T-California-San Diego', '3T-Stanford', '5T- Georgia', '5T-Texas', '5T-Washington', '5T-Wisconsin', '9T-Michigan', '9T-Michigan State', '11T-Carleton College', '11T-Harvarrd', '13T-British Columbia', '13T-Pittsburgh', '15T-North Carolina', '15T-Queen’s']\n",
      "\n",
      "Standings for 2006:\n",
      "['1-Florida', '2-Wisconsin', '3T-Georgia', '3T-Stanford', '5T-California-San Diego', '5T-Colorado', '5T-Oregon', '5T-Texas', '9T-Michigan', '9T-Michigan State', '11T-California-Santa Barbara', '11T-Kansas', '13T-Harvard', '13T-Pittsburgh', '15T-Brown', '15T-Delaware']\n",
      "\n",
      "Standings for 2007:\n",
      "['1-Wisconsin', '2-Colorado', '3T-Florida', '3T-Stanford', '5T-Carleton College', '5T-Georgia', '5T-Oregon', '5T-Texas', '9T-Ohio State', '9T-Pittsburgh', '11T-Brown', '11T-Indiana', '13T-North Carolina', '13T-Williams', '15T-Delaware', '15T-Kansas']\n",
      "\n",
      "Standings for 2008:\n",
      "['1- Wisconsin', '2-Florida', '3T-Carleton College', '3T-Colorado', '5T-Arizona', '5T-Georgia', '5T-Harvard', '5T-Illinois', '9T-California-Santa Cruz', '9T-Stanford', '11T-Pittsburgh', '11T-Texas', '13T-Dartmouth', '13T-Michigan', '15T-Delaware', '15T-North Texas']\n",
      "\n",
      "Standings for 2009:\n",
      "['1-Carleton College', '2-Colorado', '3T-Stanford', '3T-Texas', '5T-Michigan', '5T-Pittsburgh', '5T-Virginia', '5T-Wisconsin', '9T-Cornell', '9T-Williams', '11T-Luther', '11T-Tufts', '13T-California-San Diego', '13T-Minnesota', '15T-California-Santa Barbara', '15T-North Carolina State', '17T-Illinois', '17T-Kansas', '19T-California', '19T-Georgia']\n",
      "\n",
      "Standings for 2010:\n",
      "['1-Florida', '2-Carleton College', '3T-Cornell', '3T-Pittsburgh', '5T-California', '5T-Michigan', '5T-Minnesota', '5T-North Carolina-Wilmington', '9T-California-Santa Barbara', '9T-Iowa', '11T-Middlebury', '11T-Wisconsin', '13T-Colorado', '14T-Georgia', '14T-Oregon', '16-Harvard', '17T-California-San Diego', '17T-Illinois', '19T-Kansas', '19T-Texas State']\n",
      "\n",
      "Standings for 2011:\n",
      "['1-Carleton College', '2-Wisconsin', '3T-Colorado', '3T-Iowa', '5T-Harvard', '5T-Oregon', '5T-Pittsburgh', '5T- Stanford', '9-British Columbia', '10-Tufts', '11-Washington', '12-Luther', '13T-Texas', '13T-Virginia', '15T-Cornell', '15T-Whitman', '17-Illinois', '18T-California-Santa Crus\\\\z', '18T-Colorado College', '18T-Florida']\n",
      "\n",
      "Standings for 2012:\n",
      "['1- Pittsburgh', '2- Wisconsin', '3T- Oregon', '3T- Carleton College', '5T- Central Florida', '5T- Luther', '5T- Minnesota', '5T- Tufts', '9- Colorado', '10- Texas', '11-California', '12-Georgia Tech', '13T- North Carolina', '13T- Washington', '15T- Michigan', '15T-Minnesota- Duluth', '17T- Michigan State', '17T- Ohio', '19T- California-Davis', '19T- Cornell']\n",
      "\n",
      "Standings for 2013:\n",
      "['1- Pittsburgh', '2- Central Florida', '3T- Carleton College', '3T- Oregon', '5T- California- Davis', '5T- Dartmouth', '5T- North Carolina', '5T-North Carolina- Wilmington', '9T- Colorado', '9T-Harvard', '9T- Texas', '9T- Wisconsin', '13T- Arizona', '13T- Cornell', '13T- Luther', '13T- Washington', '17T- Florida State', '17T- Georgia', '17T- Illinois', '17T- Ohio']\n",
      "\n",
      "Standings for 2014:\n",
      "['1- Colorado', '2- North Carolina', '3T- North Carolina- Wilmington', '3T- Oregon', '5T- Michigan', '5T- Pittsburgh', '5T- Texas', '5T- Wisconsin', '9T- Carleton College', '9T- Central Florida', '9T- Harvard', '9T- Tufts', '13T- California- San Diego', '13T- Florida', '13T- Florida State', '13T- Massachusetts', '17T- Dartmouth', '17T- Eastern Michigan', '17T-Rutgers', '17T- Texas A&M']\n",
      "\n",
      "Standings for 2015:\n",
      "['1- North Carolina', '2- Oregon', '3T- Central Florida', '3T- Florida State', '5T- Colorado', '5T- Massachusetts', '5T- Pittsburgh', '5T- Texas', '9T- Georgia', '9T- Minnesota', '9T- North Carolina- Wilmington', '9T- Texas A&M', '13T- Auburn', '13T- Illinois', '13T- Western Washington', '13T- Wisconsin', '17T- California- Santa Barbara', '17T- Cincinnati', '17T- Cornell', '17T- Maryland']\n",
      "\n",
      "Standings for 2016:\n",
      "['1- Minnesota', '2- Harvard', 'T3- North Carolina', 'T3- Pittsburgh', 'T5- Auburn', 'T5- Colorado', 'T5- Georgia', 'T5- Wisconsin', 'T9- Massachusetts', 'T9- Michigan', 'T9- Oregon', 'T9- Texas A&M', 'T13- Carleton College', 'T13- Connecticut', 'T13- North Carolina- Wilmington', 'T13- Utah', 'T17- Cal Poly- SLO', 'T17- Case Western Reserve', 'T17- Florida State', 'T17- Washington']\n",
      "\n",
      "Standings for 2017:\n",
      "['1 – Carleton College', '2 – North Carolina-Wilmington', 'T3 – Massachussets', 'T3 – North Carolina', 'T5 – Minnesota', 'T5 – Pittsburgh', 'T5 – Washington', 'T5 – Wisconsin', 'T9 – Auburn', 'T9 – Cal Poly-SLO', 'T9 – Michigan', 'T9 – Oregon', 'T13 – British Columbia', 'T13 – Connecticut', 'T13 – Stanford', 'T13 – Texas A&M', 'T17 – Colorado', 'T17 – Colorado State', 'T17 – Oregon State', 'T17 – Virginia Tech']\n",
      "\n",
      "Standings for 2018:\n",
      "['1 – North Carolina', '2 – Pittsburgh', 'T3 – Carleton College', 'T3 – Oregon', 'T5 – Brown', 'T5 – Georgia', 'T5 – Washington', 'T5 – Wisconsin', 'T9 – Massachusetts', 'T9 – Minnesota', 'T9 – Northwestern', 'T9 – Texas', 'T13 – Auburn', 'T13 – Connecticut', 'T13 – Florida State', 'T13 – William & Mary', 'T17 – Georgetown', 'T17 – Maryland', 'T17 – Stanford', 'T17 – Victoria']\n",
      "\n",
      "Standings for 2019:\n",
      "['1 – Brown', '2 – North Carolina', 'T3 – Cal Poly-SLO', 'T3 – Colorado', 'T5 – Pittsburgh', 'T5 – Ohio State', 'T5 – Wisconsin', 'T5 – Michigan', 'T9 – Oregon', 'T9 – Washington', 'T9 – Texas', 'T9 – Georgia', 'T13 – Minnesota', 'T13 – Tufts', 'T13 – Northeastern', 'T13 – Iowa State', 'T17 – North Carolina State', 'T17 – California', 'T17 – Victoria', 'T17 – Rutgers']\n",
      "\n",
      "Standings for 2021:\n",
      "['1 – North Carolina', '2 – Georgia', 'T3 – Brown', 'T3 – Michigan', 'T5 – Washington', 'T5 – Colorado', 'T5 – Cal Poly-SLO', 'T5 – Texas', 'T9 – Ohio State', 'T9 –North Carolina-Wilmington', 'T11 – North Carolina State', 'T11 – Tulane', '13 – Northeastern', '14 – Pittsburgh', 'T15 – Wisconsin', 'T15 – Utah State', '17 – Illinois', '18 – Cornell', 'DNF – California-San Diego', 'DNF – Carleton-CUT']\n",
      "\n",
      "Standings for 2022:\n",
      "['1 – North Carolina', '2 – Brown', 'T3 – Colorado', 'T3 –Pittsburgh', 'T5 – Minnesota', 'T5 – California', 'T5 – Georgia', 'T5 – Cal Poly-SLO', 'T9 – Vermont', 'T9 – Washington', 'T9 – Texas', 'T9 – Auburn', 'T13 – Utah State', 'T13 – Wisconsin', 'T15 – Michigan', 'T15 – North Carolina State', 'T17 – William & Mary', 'T17 – Washington University', 'T19 – Connecticut', 'T19 – Ohio State']\n",
      "\n",
      "Standings for 2023:\n",
      "['1 – North Carolina', '2 – Massachusetts', 'T3 – Cal Poly-SLO', 'T3 – Vermont', 'T5 – Brown', 'T5 – Oregon', 'T5 – Pittsburgh', 'T5 – Texas', 'T9 – California', 'T9 – Carleton College', 'T9 – Colorado', 'T9 – Georgia', 'T13 – California-Santa Cruz', 'T13 – Minnesota', 'T15 – Michigan', 'T15 – Washington', 'T17 – Tufts', 'T17 – Utah State', 'T19 – Cornell', 'T19 – North Carolina State']\n"
     ]
    }
   ],
   "source": [
    "for url, standings in all_standings.items():\n",
    "    print(f\"\\nStandings for {url}:\")\n",
    "    print(standings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022 standings after fix:\n",
      "1 – North Carolina\n",
      "2 – Brown\n",
      "T3 – Colorado\n",
      "T3 – Pittsburgh\n",
      "T5 – Minnesota\n",
      "T5 – California\n",
      "T5 – Georgia\n",
      "T5 – Cal Poly-SLO\n",
      "T9 – Vermont\n",
      "T9 – Washington\n",
      "T9 – Texas\n",
      "T9 – Auburn\n",
      "T13 – Utah State\n",
      "T13 – Wisconsin\n",
      "T15 – Michigan\n",
      "T15 – North Carolina State\n",
      "T17 – William & Mary\n",
      "T17 – Washington University\n",
      "T19 – Connecticut\n",
      "T19 – Ohio State\n"
     ]
    }
   ],
   "source": [
    "# Fix Pittsburgh entry in 2022 standings\n",
    "if 2022 in all_standings:\n",
    "    all_standings[2022] = [\n",
    "        entry.replace('T3 –Pittsburgh', 'T3 – Pittsburgh')  # Note the en dash (–) not hyphen (-)\n",
    "        for entry in all_standings[2022]\n",
    "    ]\n",
    "\n",
    "# Verify the fix\n",
    "print(\"2022 standings after fix:\")\n",
    "for entry in all_standings[2022]:\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty lists to store our data\n",
    "data = []\n",
    "\n",
    "# Iterate through the standings dictionary\n",
    "for year, standings in all_standings.items():\n",
    "    if isinstance(standings, list):  # Only process if we have actual standings data\n",
    "        for i, entry in enumerate(standings, 1):\n",
    "            if year > 2016:\n",
    "                # Try both types of dashes for newer years\n",
    "                if ' – ' in entry:  # en dash\n",
    "                    parts = entry.split(' – ', 1)\n",
    "                elif ' - ' in entry:  # regular hyphen\n",
    "                    parts = entry.split(' - ', 1)\n",
    "                else:\n",
    "                    parts = entry.split('-', 1)\n",
    "            else:\n",
    "                # Pre-2016 format\n",
    "                parts = entry.split('-', 1)\n",
    "            \n",
    "            if len(parts) == 2:\n",
    "                web_finish = parts[0].strip()\n",
    "                team = parts[1].strip()\n",
    "                \n",
    "                data.append({\n",
    "                    'URL': urls[year],\n",
    "                    'Year': year,\n",
    "                    'Team': team,\n",
    "                    'Web_Finish': web_finish,\n",
    "                    'List_Finish': i\n",
    "                })\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data exported to 'ultimate_standings_raw.csv'\n"
     ]
    }
   ],
   "source": [
    "# Export to CSV\n",
    "df.to_csv('ultimate_standings_raw.csv', index=False)\n",
    "print(\"Data exported to 'ultimate_standings_raw.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All raw teams in alphabetical order:\n",
      "Arizona\n",
      "Auburn\n",
      "Boston College\n",
      "British Columbia\n",
      "Brown\n",
      "Cal Poly SLO\n",
      "Cal Poly- SLO\n",
      "Cal Poly-SLO\n",
      "California\n",
      "California -Santa Cruz\n",
      "California- Davis\n",
      "California- San Diego\n",
      "California- Santa Barbara\n",
      "California- Santa Cruz\n",
      "California-Davis\n",
      "California-San Diego\n",
      "California-Santa Barbara\n",
      "California-Santa Crus\\z\n",
      "California-Santa Cruz\n",
      "Carleton\n",
      "Carleton College\n",
      "Carleton-CUT\n",
      "Carnegie Mellon\n",
      "Case Western Reserve\n",
      "Central Florida\n",
      "Chabot Community College\n",
      "Cincinnati\n",
      "Colorado\n",
      "Colorado College\n",
      "Colorado State\n",
      "Columbia\n",
      "Connecticut\n",
      "Cornell\n",
      "Dartmouth\n",
      "Delaware\n",
      "Duke\n",
      "East Carolina\n",
      "Eastern Michigan\n",
      "Florida\n",
      "Florida State\n",
      "George Washington\n",
      "Georgetown\n",
      "Georgia\n",
      "Georgia Tech\n",
      "Glassboro\n",
      "Harvard\n",
      "Harvarrd\n",
      "Illinois\n",
      "Indiana\n",
      "Iowa\n",
      "Iowa State\n",
      "Kansas\n",
      "LSU\n",
      "Las Positas\n",
      "Luther\n",
      "MIT\n",
      "Maryland\n",
      "Massachusetts\n",
      "Massachussets\n",
      "Michigan\n",
      "Michigan State\n",
      "Middlebury\n",
      "Minnesota\n",
      "Minnesota- Duluth\n",
      "North Carolina\n",
      "North Carolina State\n",
      "North Carolina- Wilmington\n",
      "North Carolina-Wilmington\n",
      "North Texas\n",
      "Northeastern\n",
      "Northwestern\n",
      "Notre Dame\n",
      "Oberlin\n",
      "Ohio\n",
      "Ohio State\n",
      "Oregon\n",
      "Oregon State\n",
      "Penn State\n",
      "Pennsylvania\n",
      "Pittsburgh\n",
      "Princeton\n",
      "Queen’s\n",
      "Rice\n",
      "Rutgers\n",
      "SUNY-Albany\n",
      "SUNY-Binghampton\n",
      "SUNY-Purchase\n",
      "SW Missouri State\n",
      "Saint Louis\n",
      "Salisbury\n",
      "Stanford\n",
      "Swarthmore\n",
      "Syracuse\n",
      "Texas\n",
      "Texas A&M\n",
      "Texas State\n",
      "Tufts\n",
      "Tulane\n",
      "Utah\n",
      "Utah State\n",
      "Vermont\n",
      "Victoria\n",
      "Virginia\n",
      "Virginia Tech\n",
      "Washington\n",
      "Washington University\n",
      "Wesleyan\n",
      "Western Washington\n",
      "Whitman\n",
      "William & Mary\n",
      "William and Mary\n",
      "Williams\n",
      "Wilmington\n",
      "Winona State\n",
      "Wisconsin\n",
      "Yale\n"
     ]
    }
   ],
   "source": [
    "# Get unique team names and sort alphabetically\n",
    "unique_teams_raw = sorted(df['Team'].unique())\n",
    "\n",
    "# Print the teams\n",
    "print(\"All raw teams in alphabetical order:\")\n",
    "for team in unique_teams_raw:\n",
    "    print(team)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_team_name(name):\n",
    "    # First, strip whitespace and convert to title case\n",
    "    name = name.strip().title()\n",
    "    \n",
    "    # Fix specific typos\n",
    "    name = name.replace(\"Harvarrd\", \"Harvard\")\n",
    "    name = name.replace(\"California-Santa Crus\\\\z\", \"California-Santa Cruz\")\n",
    "    name = name.replace(\"Massachussets\", \"Massachusetts\")\n",
    "    name = name.replace(\"Sw Missouri State\", \"SW Missouri State\")\n",
    "    \n",
    "    # Standardize hyphenation\n",
    "    name = name.replace(\" - \", \"-\")\n",
    "    name = name.replace(\"- \", \"-\")\n",
    "    name = name.replace(\" -\", \"-\")\n",
    "    \n",
    "    # Standardize common school names\n",
    "    replacements = {\n",
    "        \"California-Santa Barbara\": \"UC Santa Barbara\",\n",
    "        \"California-Davis\": \"UC Davis\",\n",
    "        \"California-San Diego\": \"UC San Diego\",\n",
    "        \"California-Santa Cruz\": \"UC Santa Cruz\",\n",
    "        \"California-Santa Crus\\Z\": \"UC Santa Cruz\",\n",
    "        \"Cal Poly Slo\": \"Cal Poly SLO\",\n",
    "        \"Cal Poly-Slo\": \"Cal Poly SLO\",\n",
    "        \"Cal Poly- Slo\": \"Cal Poly SLO\",\n",
    "        \"North Carolina-Wilmington\": \"UNC Wilmington\",\n",
    "        \"North Carolina- Wilmington\": \"UNC Wilmington\",\n",
    "        #\"Massachusetts\": \"UMass\",\n",
    "        \"Carleton College\": \"Carleton\",\n",
    "        \"Carleton-Cut\": \"Carleton\",\n",
    "        \"William and Mary\": \"William & Mary\",\n",
    "        \"William And Mary\": \"William & Mary\",\n",
    "        \"Washington University\": \"WashU\",\n",
    "        \"Pennsylvania\": \"Penn\",\n",
    "        \"Southwest Missouri State\": \"SW Missouri State\",\n",
    "        \"Las Positas\": \"Las Positas College\",\n",
    "        \"Suny-\": \"SUNY-\",  # Ensure SUNY prefix is capitalized\n",
    "        \"North Carolina State\": \"NC State\",\n",
    "        \"Queen’S\": \"Queen's\",\n",
    "        \"Mit\":\"MIT\",\n",
    "        \"Lsu\":\"LSU\",\n",
    "        \"Wilmington\":\"UNC Wilmington\",\n",
    "        \"North Carolina-Wilmington\":\"UNC Wilmington\",\n",
    "        \"Suny-Purchase\":\"SUNY Purchase\",\n",
    "        \"Suny-Binghamton\":\"SUNY Binghamton\",\n",
    "        \"Suny-Albany\":\"SUNY Albany\",\n",
    "        \"SUNY-Albany\":\"SUNY Albany\",\n",
    "        \"SUNY-Binghampton\":\"SUNY Binghamton\",\n",
    "        \"SUNY-Purchase\":\"SUNY Purchase\"\n",
    "    }\n",
    "    \n",
    "    for old, new in replacements.items():\n",
    "        if name.upper() == old.upper():  # Case-insensitive replacement\n",
    "            name = new\n",
    "    \n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All teams after cleaning (alphabetically):\n",
      "Arizona\n",
      "Auburn\n",
      "Boston College\n",
      "British Columbia\n",
      "Brown\n",
      "Cal Poly SLO\n",
      "California\n",
      "Carleton\n",
      "Carnegie Mellon\n",
      "Case Western Reserve\n",
      "Central Florida\n",
      "Chabot Community College\n",
      "Cincinnati\n",
      "Colorado\n",
      "Colorado College\n",
      "Colorado State\n",
      "Columbia\n",
      "Connecticut\n",
      "Cornell\n",
      "Dartmouth\n",
      "Delaware\n",
      "Duke\n",
      "East Carolina\n",
      "Eastern Michigan\n",
      "Florida\n",
      "Florida State\n",
      "George Washington\n",
      "Georgetown\n",
      "Georgia\n",
      "Georgia Tech\n",
      "Glassboro\n",
      "Harvard\n",
      "Illinois\n",
      "Indiana\n",
      "Iowa\n",
      "Iowa State\n",
      "Kansas\n",
      "LSU\n",
      "Las Positas College\n",
      "Luther\n",
      "MIT\n",
      "Maryland\n",
      "Massachusetts\n",
      "Michigan\n",
      "Michigan State\n",
      "Middlebury\n",
      "Minnesota\n",
      "Minnesota-Duluth\n",
      "NC State\n",
      "North Carolina\n",
      "North Texas\n",
      "Northeastern\n",
      "Northwestern\n",
      "Notre Dame\n",
      "Oberlin\n",
      "Ohio\n",
      "Ohio State\n",
      "Oregon\n",
      "Oregon State\n",
      "Penn\n",
      "Penn State\n",
      "Pittsburgh\n",
      "Princeton\n",
      "Queen's\n",
      "Rice\n",
      "Rutgers\n",
      "SUNY Albany\n",
      "SUNY Binghamton\n",
      "SUNY Purchase\n",
      "SW Missouri State\n",
      "Saint Louis\n",
      "Salisbury\n",
      "Stanford\n",
      "Swarthmore\n",
      "Syracuse\n",
      "Texas\n",
      "Texas A&M\n",
      "Texas State\n",
      "Tufts\n",
      "Tulane\n",
      "UC Davis\n",
      "UC San Diego\n",
      "UC Santa Barbara\n",
      "UC Santa Cruz\n",
      "UNC Wilmington\n",
      "Utah\n",
      "Utah State\n",
      "Vermont\n",
      "Victoria\n",
      "Virginia\n",
      "Virginia Tech\n",
      "WashU\n",
      "Washington\n",
      "Wesleyan\n",
      "Western Washington\n",
      "Whitman\n",
      "William & Mary\n",
      "Williams\n",
      "Winona State\n",
      "Wisconsin\n",
      "Yale\n"
     ]
    }
   ],
   "source": [
    "# Apply the cleaning function to create new column\n",
    "df['Team_Clean'] = df['Team'].apply(clean_team_name)\n",
    "\n",
    "# Print unique teams after cleaning to verify changes\n",
    "print(\"All teams after cleaning (alphabetically):\")\n",
    "for team in sorted(df['Team_Clean'].unique()):\n",
    "    print(team)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Year</th>\n",
       "      <th>Team</th>\n",
       "      <th>Web_Finish</th>\n",
       "      <th>List_Finish</th>\n",
       "      <th>Team_Clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://collegechampionships.usaultimate.org/d...</td>\n",
       "      <td>1984</td>\n",
       "      <td>Stanford</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Stanford</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://collegechampionships.usaultimate.org/d...</td>\n",
       "      <td>1984</td>\n",
       "      <td>Glassboro</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Glassboro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://collegechampionships.usaultimate.org/d...</td>\n",
       "      <td>1984</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>3T</td>\n",
       "      <td>3</td>\n",
       "      <td>Massachusetts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://collegechampionships.usaultimate.org/d...</td>\n",
       "      <td>1984</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>3T</td>\n",
       "      <td>4</td>\n",
       "      <td>Penn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://collegechampionships.usaultimate.org/d...</td>\n",
       "      <td>1984</td>\n",
       "      <td>Chabot Community College</td>\n",
       "      <td>?</td>\n",
       "      <td>5</td>\n",
       "      <td>Chabot Community College</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>https://collegechampionships.usaultimate.org/d...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Washington</td>\n",
       "      <td>T15</td>\n",
       "      <td>16</td>\n",
       "      <td>Washington</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616</th>\n",
       "      <td>https://collegechampionships.usaultimate.org/d...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Tufts</td>\n",
       "      <td>T17</td>\n",
       "      <td>17</td>\n",
       "      <td>Tufts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617</th>\n",
       "      <td>https://collegechampionships.usaultimate.org/d...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Utah State</td>\n",
       "      <td>T17</td>\n",
       "      <td>18</td>\n",
       "      <td>Utah State</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>https://collegechampionships.usaultimate.org/d...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Cornell</td>\n",
       "      <td>T19</td>\n",
       "      <td>19</td>\n",
       "      <td>Cornell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>https://collegechampionships.usaultimate.org/d...</td>\n",
       "      <td>2023</td>\n",
       "      <td>North Carolina State</td>\n",
       "      <td>T19</td>\n",
       "      <td>20</td>\n",
       "      <td>NC State</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>620 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   URL  Year  \\\n",
       "0    https://collegechampionships.usaultimate.org/d...  1984   \n",
       "1    https://collegechampionships.usaultimate.org/d...  1984   \n",
       "2    https://collegechampionships.usaultimate.org/d...  1984   \n",
       "3    https://collegechampionships.usaultimate.org/d...  1984   \n",
       "4    https://collegechampionships.usaultimate.org/d...  1984   \n",
       "..                                                 ...   ...   \n",
       "615  https://collegechampionships.usaultimate.org/d...  2023   \n",
       "616  https://collegechampionships.usaultimate.org/d...  2023   \n",
       "617  https://collegechampionships.usaultimate.org/d...  2023   \n",
       "618  https://collegechampionships.usaultimate.org/d...  2023   \n",
       "619  https://collegechampionships.usaultimate.org/d...  2023   \n",
       "\n",
       "                         Team Web_Finish  List_Finish  \\\n",
       "0                    Stanford          1            1   \n",
       "1                   Glassboro          2            2   \n",
       "2               Massachusetts         3T            3   \n",
       "3                Pennsylvania         3T            4   \n",
       "4    Chabot Community College          ?            5   \n",
       "..                        ...        ...          ...   \n",
       "615                Washington        T15           16   \n",
       "616                     Tufts        T17           17   \n",
       "617                Utah State        T17           18   \n",
       "618                   Cornell        T19           19   \n",
       "619      North Carolina State        T19           20   \n",
       "\n",
       "                   Team_Clean  \n",
       "0                    Stanford  \n",
       "1                   Glassboro  \n",
       "2               Massachusetts  \n",
       "3                        Penn  \n",
       "4    Chabot Community College  \n",
       "..                        ...  \n",
       "615                Washington  \n",
       "616                     Tufts  \n",
       "617                Utah State  \n",
       "618                   Cornell  \n",
       "619                  NC State  \n",
       "\n",
       "[620 rows x 6 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create new dataframe without URL and Team columns\n",
    "# df_clean = df.drop(['URL', 'Team', 'Web_Finish'], axis=1)\n",
    "\n",
    "# # Rename Team_Clean to Team for clarity\n",
    "# df_clean = df_clean.rename(columns={'Team_Clean': 'Team', 'List_Finish': 'Rank'})\n",
    "\n",
    "# # Create Time column accounting for missing 2020 season\n",
    "# df_clean['Time'] = df_clean.apply(lambda row: \n",
    "#     row['Year'] - 1983 if row['Year'] < 2020 \n",
    "#     else row['Year'] - 1984, axis=1)\n",
    "\n",
    "# # Reorder columns\n",
    "# df_clean = df_clean[['Team', 'Time', 'Rank', 'Year']]\n",
    "\n",
    "# # Verify the first few rows to check the mapping\n",
    "# df_clean\n",
    "\n",
    "# # Export to CSV\n",
    "# df_clean.to_csv('ultimate_standings_clean.csv', index=False)\n",
    "# print(\"Data exported to 'ultimate_standings_clean.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_rank_format(df, column_name):\n",
    "    def convert_format(rank):\n",
    "        # If it's already in T# format, keep it\n",
    "        if isinstance(rank, str) and rank.startswith('T'):\n",
    "            return rank\n",
    "        \n",
    "        # If it ends with T, convert to T# format\n",
    "        if isinstance(rank, str) and rank.endswith('T'):\n",
    "            number = rank[:-1]  # Remove the T\n",
    "            return f'T{number}'\n",
    "        \n",
    "        return rank  # Return unchanged if no T\n",
    "\n",
    "    # Apply the conversion to the specified column\n",
    "    df[column_name] = df[column_name].apply(convert_format)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data exported to 'ultimate_standings_T.csv'\n"
     ]
    }
   ],
   "source": [
    "# Create new dataframe without URL and Team columns\n",
    "df_T = df.drop(['URL', 'Team'], axis=1)\n",
    "\n",
    "# Apply the conversion to the specified column\n",
    "df = transform_rank_format(df_T, 'Web_Finish')\n",
    "\n",
    "# Rename Team_Clean to Team for clarity\n",
    "df_T = df_T.rename(columns={'Team_Clean': 'Team', 'List_Finish': 'Rank', 'Web_Finish': 'T_Rank'})\n",
    "\n",
    "# Reorder columns\n",
    "df_T = df_T[['Team', 'Year', 'Rank', 'T_Rank']]\n",
    "\n",
    "# Manually change a rank value for UNC Wilmington in 2021\n",
    "df_T.loc[(df_T['Team'] == 'UNC Wilmington') & (df_T['Year'] == 2021), 'T_Rank'] = 'T9'\n",
    "\n",
    "# Verify the first few rows to check the mapping\n",
    "df_T\n",
    "\n",
    "# Export to CSV\n",
    "df_T.to_csv('ultimate_standings_T.csv', index=False)\n",
    "print(\"Data exported to 'ultimate_standings_T.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read the CSV\n",
    "# df_with_less_columns_for_tableau = pd.read_csv('ultimate_standings_clean.csv')\n",
    "\n",
    "# # Group by year and check rank sequences\n",
    "# found_issues = False\n",
    "# for year in sorted(df_with_less_columns_for_tableau['Year'].unique()):\n",
    "#     year_data = df_with_less_columns_for_tableau[df_with_less_columns_for_tableau['Year'] == year].sort_values('Rank')\n",
    "#     ranks = year_data['Rank'].tolist()\n",
    "#     expected_ranks = list(range(1, len(ranks) + 1))\n",
    "    \n",
    "#     if ranks != expected_ranks:\n",
    "#         found_issues = True\n",
    "#         print(f\"\\nYear {year} has non-sequential ranks:\")\n",
    "#         print(\"Expected:\", expected_ranks)\n",
    "#         print(\"Actual:\", ranks)\n",
    "#         print(\"\\nFull data for this year:\")\n",
    "#         print(year_data[['Team', 'Rank']].to_string())\n",
    "\n",
    "# if not found_issues:\n",
    "#     print(\"All years have sequential ranks without gaps! ✓\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Web_Finish</th>\n",
       "      <th>List_Finish</th>\n",
       "      <th>Team_Clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1984</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Stanford</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1984</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Glassboro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1984</td>\n",
       "      <td>T3</td>\n",
       "      <td>3</td>\n",
       "      <td>Massachusetts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1984</td>\n",
       "      <td>T3</td>\n",
       "      <td>4</td>\n",
       "      <td>Penn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1984</td>\n",
       "      <td>?</td>\n",
       "      <td>5</td>\n",
       "      <td>Chabot Community College</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>2023</td>\n",
       "      <td>T15</td>\n",
       "      <td>16</td>\n",
       "      <td>Washington</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616</th>\n",
       "      <td>2023</td>\n",
       "      <td>T17</td>\n",
       "      <td>17</td>\n",
       "      <td>Tufts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617</th>\n",
       "      <td>2023</td>\n",
       "      <td>T17</td>\n",
       "      <td>18</td>\n",
       "      <td>Utah State</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>2023</td>\n",
       "      <td>T19</td>\n",
       "      <td>19</td>\n",
       "      <td>Cornell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>2023</td>\n",
       "      <td>T19</td>\n",
       "      <td>20</td>\n",
       "      <td>NC State</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>620 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Year Web_Finish  List_Finish                Team_Clean\n",
       "0    1984          1            1                  Stanford\n",
       "1    1984          2            2                 Glassboro\n",
       "2    1984         T3            3             Massachusetts\n",
       "3    1984         T3            4                      Penn\n",
       "4    1984          ?            5  Chabot Community College\n",
       "..    ...        ...          ...                       ...\n",
       "615  2023        T15           16                Washington\n",
       "616  2023        T17           17                     Tufts\n",
       "617  2023        T17           18                Utah State\n",
       "618  2023        T19           19                   Cornell\n",
       "619  2023        T19           20                  NC State\n",
       "\n",
       "[620 rows x 4 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make a big df with logo urls\n",
    "\n",
    "# # Read in the csv with the team names and urls\n",
    "# team_logos_df = pd.read_csv('team_logos.csv')\n",
    "\n",
    "# team_logos_df\n",
    "\n",
    "# # Remove duplicates from team_logos_df before merging\n",
    "# team_logos_df_unique = team_logos_df.drop_duplicates('team_name')\n",
    "\n",
    "# team_logos_df_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Merge the two dfs on the team name\n",
    "\n",
    "# # This will:\n",
    "# # Match rows where df['Team_Clean'] equals team_logos_df['team_name']\n",
    "# # Keep all rows from df (due to left join)\n",
    "# # Add all columns from team_logos_df\n",
    "# # Optionally remove the duplicate team name column\n",
    "\n",
    "# # Merge using left_on and right_on to specify the different column names\n",
    "# df_merged = pd.merge(\n",
    "#     df, \n",
    "#     team_logos_df_unique, \n",
    "#     left_on='Team_Clean',\n",
    "#     right_on='team_nam\n",
    "#     how='left'\n",
    "# )\n",
    "\n",
    "# df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Export the merged DataFrame to a CSV file\n",
    "# df_merged.to_csv('ultimate_standings_merged.csv', index=False)\n",
    "# print(\"Data exported to 'ultimate_standings_merged.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add missing logos manually\n",
    "\n",
    "# teams_with_missing_logos = [\n",
    "#     #'Arizona',\n",
    "#     #'Boston College',\n",
    "#     #'Carnegie Mellon',\n",
    "#     #'Case Western Reserve',\n",
    "#     #'Chabot Community College',\n",
    "#     'Colorado College',\n",
    "#     'Cornell',\n",
    "#     'Dartmouth',\n",
    "#     'Delaware',\n",
    "#     'East Carolina',\n",
    "#     'Eastern Michigan',\n",
    "#     'Florida State',\n",
    "#     'George Washington',\n",
    "#     'Glassboro',\n",
    "#     'Indiana',\n",
    "#     'Iowa',\n",
    "#     'Kansas',\n",
    "#     'Las Positas College',\n",
    "#     'Luther',\n",
    "#     'Michigan State',\n",
    "#     'Middlebury',\n",
    "#     'Minnesota-Duluth',\n",
    "#     'North Texas',\n",
    "#     'Notre Dame',\n",
    "#     'Oberlin',\n",
    "#     'Ohio',\n",
    "#     'Penn',\n",
    "#     'Princeton',\n",
    "#     \"Queen's\",\n",
    "#     'Rice',\n",
    "#     'Rutgers',\n",
    "#     'Saint Louis',\n",
    "#     'Salisbury',\n",
    "#     'Suny-Albany',\n",
    "#     'Suny-Binghampton',\n",
    "#     'Suny-Purchase',\n",
    "#     'Sw Missouri State',\n",
    "#     'Swarthmore',\n",
    "#     'Syracuse',\n",
    "#     'Texas State',\n",
    "#     'UC Davis',\n",
    "#     'Utah',\n",
    "#     'Virginia',\n",
    "#     'Wesleyan',\n",
    "#     'Whitman',\n",
    "#     'Williams',\n",
    "#     'Wilmington',\n",
    "#     'Winona State',\n",
    "#     'Yale'\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get unique teams and sort them\n",
    "# unique_teams = sorted(df_merged['Team_Clean'].unique())\n",
    "\n",
    "\n",
    "# # Print them to verify\n",
    "# for team in unique_teams:\n",
    "#     print(team)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding team results manually, for 2024\n",
    "\n",
    "# Read in the existing CSV\n",
    "df_T = pd.read_csv('ultimate_standings_T.csv')\n",
    "\n",
    "# Remove any existing 2024 entries\n",
    "df_T = df_T[df_T['Year'] != 2024]\n",
    "\n",
    "# Create new data\n",
    "new_data = {\n",
    "    'Team': [\n",
    "        'Brown', 'Cal Poly SLO', 'Colorado', 'North Carolina', 'Georgia',\n",
    "        'Minnesota', 'NC State', 'Oregon', 'Massachusetts', 'Pittsburgh',\n",
    "        'Texas', 'Michigan', 'Oregon State', 'California', 'Vermont',\n",
    "        'WashU', 'Penn State', 'Carleton', 'Alabama-Huntsville', 'Ottawa'\n",
    "    ],\n",
    "    'Year': [2024] * 20,\n",
    "    'Rank': list(range(1, 21)),\n",
    "    'T_Rank': [\n",
    "        '1', '2', 'T3', 'T3', 'T5', 'T5', 'T5', 'T5', 'T9', 'T9',\n",
    "        'T9', 'T9', 'T13', 'T13', 'T15', 'T15', 'T17', 'T17', 'T19', 'T19'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "new_df = pd.DataFrame(new_data)\n",
    "\n",
    "# Append to existing DataFrame\n",
    "df_T = pd.concat([df_T, new_df], ignore_index=True)\n",
    "\n",
    "# Export to CSV\n",
    "df_T.to_csv('ultimate_standings_T.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teams in men's but not women's:\n",
      "{'Texas State', 'Auburn', 'Cincinnati', 'George Washington', 'Boston College', 'Case Western Reserve', 'Oregon State', 'WashU', 'LSU', 'Utah State', \"Queen's\", 'Virginia Tech', 'Penn', 'UNC Wilmington', 'SW Missouri State', 'Glassboro', 'Salisbury', 'Minnesota-Duluth', 'NC State', 'Alabama-Huntsville', 'Eastern Michigan', 'Chabot Community College', 'Tulane', 'Cal Poly SLO', 'Winona State', 'Georgetown', 'SUNY Albany', 'Syracuse', 'North Texas', 'Las Positas College', 'Luther', 'SUNY Purchase'}\n",
      "\n",
      "Teams in women's but not men's:\n",
      "{'Purdue', 'Sonoma State', 'Claremont', 'North Carolina- Wilmington', 'Smith', 'Texas-Dallas', 'Southern California', 'North Carolina-Wilmington', 'Bucknell', 'West Chester', 'North Carolina State', 'UCLA', 'Truman State', 'Washington University', 'Emory', 'Towson', 'Earlham', 'Pennsylvania', 'Wake Forest', 'Humboldt State', 'Chicago', 'SUNY-Binghamton', 'NYU', 'Boston University'}\n",
      "\n",
      "Combined dataset shape: (1212, 5)\n",
      "\n",
      "Years covered: [1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2021, 2022, 2023, 2024]\n",
      "\n",
      "Total number of teams: 121\n"
     ]
    }
   ],
   "source": [
    "# Read in the mens and womens csvs\n",
    "df_mens = pd.read_csv('college-mens-rankings.csv')\n",
    "df_womens = pd.read_csv('college-womens-rankings.csv')\n",
    "\n",
    "# Look at the Team column and see if there are any mismatches between men and women\n",
    "mens_teams = set(df_mens['Team'].unique())\n",
    "womens_teams = set(df_womens['Team'].unique())\n",
    "\n",
    "print(\"Teams in men's but not women's:\")\n",
    "print(mens_teams - womens_teams)\n",
    "print(\"\\nTeams in women's but not men's:\")\n",
    "print(womens_teams - mens_teams)\n",
    "\n",
    "# Define standardization dictionary\n",
    "name_standardization = {\n",
    "    'North Carolina-Wilmington': 'UNC Wilmington',\n",
    "    'North Carolina- Wilmington': 'UNC Wilmington',\n",
    "    'Pennsylvania': 'Penn',\n",
    "    'Washington University': 'WashU',\n",
    "    'North Carolina State': 'NC State',\n",
    "    'Cal Poly-SLO': 'Cal Poly SLO',\n",
    "    'Cal Poly- SLO': 'Cal Poly SLO',\n",
    "    'SUNY-Binghamton': 'SUNY Binghamton',\n",
    "    'Texas-Dallas': 'UT Dallas',\n",
    "    'Southern California': 'USC'\n",
    "}\n",
    "\n",
    "# Apply standardization to both datasets\n",
    "df_mens['Team'] = df_mens['Team'].replace(name_standardization)\n",
    "df_womens['Team'] = df_womens['Team'].replace(name_standardization)\n",
    "\n",
    "# Add a 'Division' column to both\n",
    "df_mens['Division'] = \"College Men's\"\n",
    "df_womens['Division'] = \"College Women's\"\n",
    "\n",
    "# Concatenate both datasets\n",
    "df_combined = pd.concat([df_mens, df_womens], ignore_index=True)\n",
    "\n",
    "# Sort by Year, Division, and Rank\n",
    "df_combined = df_combined.sort_values(['Division','Year', 'Rank'])\n",
    "\n",
    "# Export to csv called college-rankings-combined.csv\n",
    "df_combined.to_csv('college-rankings-combined.csv', index=False)\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nCombined dataset shape:\", df_combined.shape)\n",
    "print(\"\\nYears covered:\", sorted(df_combined['Year'].unique()))\n",
    "print(\"\\nTotal number of teams:\", len(df_combined['Team'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
